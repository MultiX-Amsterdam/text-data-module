
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Tutorial (Text Data Processing) &#8212; Text Data Processing</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Assignment (Text Data Processing)" href="assignment-text-data.html" />
    <link rel="prev" title="Preparation (Text Data Processing)" href="preparation-text-data.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Text Data Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="overview-text-data.html">
                    Text Data Processing
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preparation-text-data.html">
   Preparation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="assignment-text-data.html">
   Assignment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="answer.html">
   Task Answers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="util.html">
   Utility Functions
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/MultiX-Amsterdam/text-data-module/blob/main/docs/tutorial-text-data.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/MultiX-Amsterdam/text-data-module"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/MultiX-Amsterdam/text-data-module/issues/new?title=Issue%20on%20page%20%2Fdocs/tutorial-text-data.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/docs/tutorial-text-data.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scenario">
   Scenario
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#import-packages">
   Import Packages
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#task-answers">
   Task Answers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utility-file">
   Utility File
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#task-3-preprocess-text-data">
   Task 3: Preprocess Text Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization">
     Tokenization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#part-of-speech-tagging">
     Part-of-speech tagging
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stemming-lemmatization">
     Stemming / Lemmatization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stopword-removal">
     Stopword Removal
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assignment-for-task-3-1-tokenization-and-lemmatization">
     Assignment for Task 3.1: Tokenization and Lemmatization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assignment-for-task-3-2-word-counts">
     Assignment for Task 3.2: Word Counts
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assignment-for-task-3-3-stop-words-removal">
     Assignment for Task 3.3: Stop Words Removal
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#another-option-spacy">
     Another Option: spaCy
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#task-4-unsupervised-learning-topic-modeling">
   Task 4: Unsupervised Learning - Topic Modeling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assignment-for-task-4">
     Assignment for Task 4
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#task-5-supervised-learning-topic-classification">
   Task 5: Supervised Learning - Topic Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compute-word-embeddings">
     Compute Word Embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-the-classifier">
     Build the Classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optional-assignment-for-task-5">
     Optional Assignment for Task 5
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Tutorial (Text Data Processing)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scenario">
   Scenario
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#import-packages">
   Import Packages
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#task-answers">
   Task Answers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utility-file">
   Utility File
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#task-3-preprocess-text-data">
   Task 3: Preprocess Text Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization">
     Tokenization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#part-of-speech-tagging">
     Part-of-speech tagging
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stemming-lemmatization">
     Stemming / Lemmatization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stopword-removal">
     Stopword Removal
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assignment-for-task-3-1-tokenization-and-lemmatization">
     Assignment for Task 3.1: Tokenization and Lemmatization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assignment-for-task-3-2-word-counts">
     Assignment for Task 3.2: Word Counts
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assignment-for-task-3-3-stop-words-removal">
     Assignment for Task 3.3: Stop Words Removal
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#another-option-spacy">
     Another Option: spaCy
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#task-4-unsupervised-learning-topic-modeling">
   Task 4: Unsupervised Learning - Topic Modeling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assignment-for-task-4">
     Assignment for Task 4
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#task-5-supervised-learning-topic-classification">
   Task 5: Supervised Learning - Topic Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compute-word-embeddings">
     Compute Word Embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-the-classifier">
     Build the Classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optional-assignment-for-task-5">
     Optional Assignment for Task 5
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="tutorial-text-data-processing">
<h1>Tutorial (Text Data Processing)<a class="headerlink" href="#tutorial-text-data-processing" title="Permalink to this headline">#</a></h1>
<p>(Last updated: Jan 29, 2025)<a class="footnote-reference brackets" href="#credit" id="id1">1</a></p>
<p>This tutorial will familiarize you with the data science pipeline of processing text data. We will go through the various steps involved in the Natural Language Processing (NLP) pipeline for topic modelling and topic classification, including tokenization, lemmatization, and obtaining word embeddings. We will also build a neural network using PyTorch for multi-class topic classification using the dataset.</p>
<p>The AG’s News Topic Classification Dataset contains news articles from four different categories, making it a nice source of text data for NLP tasks. We will guide you through the process of understanding the dataset, implementing various NLP techniques, and building a model for classification.</p>
<p>You can use the following links to jump to the tasks and assignments:</p>
<ul class="simple">
<li><p><a class="reference external" href="#t3">Task 3: Preprocess Text Data</a></p>
<ul>
<li><p><a class="reference external" href="#t3-1">Tokenization</a></p></li>
<li><p><a class="reference external" href="#t3-2">Part-of-speech tagging</a></p></li>
<li><p><a class="reference external" href="#t3-3">Stemming / Lemmatization</a></p></li>
<li><p><a class="reference external" href="#t3-4">Stopword Removal</a></p></li>
<li><p><a class="reference external" href="#a3-1">Assignment for Task 3.1: Tokenization and Lemmatization</a></p></li>
<li><p><a class="reference external" href="#a3-2">Assignment for Task 3.2: Word Counts</a></p></li>
<li><p><a class="reference external" href="#a3-3">Assignment for Task 3.3: Stop Words Removal</a></p></li>
<li><p><a class="reference external" href="#spacy">Another Option: spaCy</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#t4">Task 4: Unsupervised Learning - Topic Modelling</a></p>
<ul>
<li><p><a class="reference external" href="#a4">Assignment for Task 4</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#t5">Task 5: Supervised Learning - Topic Classification</a></p>
<ul>
<li><p><a class="reference external" href="#t5-1">Compute Word Embeddings</a></p></li>
<li><p><a class="reference external" href="#t5-2">Build the Classifier</a></p></li>
<li><p><a class="reference external" href="#a5">Optional Assignment for Task 5</a></p></li>
</ul>
</li>
</ul>
<section id="scenario">
<h2>Scenario<a class="headerlink" href="#scenario" title="Permalink to this headline">#</a></h2>
<p>The <a class="reference external" href="https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset">AG’s News Topic Classification Dataset</a> is a collection of over 1 million news articles from more than 2000 news sources. The dataset was created by selecting the 4 largest classes from the original corpus, resulting in 120,000 training samples and 7,600 testing samples. The dataset is provided by the academic community for research purposes in data mining, information retrieval, and other non-commercial activities. We will use it to demonstrate various NLP techniques on real data, and in the end, make 2 models with this data. The files train.csv and test.csv contain all the training and testing samples as comma-separated values with 3 columns: class index, title, and description. Download train.csv and test.csv for the following tasks.</p>
</section>
<section id="import-packages">
<h2>Import Packages<a class="headerlink" href="#import-packages" title="Permalink to this headline">#</a></h2>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>To make this notebook work, you need to install the packages by following the instructions in the <a class="reference internal" href="preparation-text-data.html"><span class="doc std std-doc">preparation</span></a> step. You can also copy this notebook (as well as the dataset) to Google Colab and run the notebook on it.</p>
</div>
<p>We put all the packages that are needed for this tutorial below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">import</span> <span class="nn">spacy</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">stopwords</span><span class="p">,</span>
    <span class="n">wordnet</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">SnowballStemmer</span><span class="p">,</span>
    <span class="n">WordNetLemmatizer</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">LatentDirichletAllocation</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">adjusted_mutual_info_score</span><span class="p">,</span>
    <span class="n">adjusted_rand_score</span><span class="p">,</span>
    <span class="n">confusion_matrix</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="nn">wordcloud</span> <span class="kn">import</span> <span class="n">WordCloud</span>

<span class="c1"># Import the answers for the tasks</span>
<span class="kn">from</span> <span class="nn">util.answer</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">answer_tokenize_and_lemmatize</span><span class="p">,</span>
    <span class="n">answer_get_word_counts</span><span class="p">,</span>
    <span class="n">answer_remove_stopwords</span><span class="p">,</span>
    <span class="n">answer_get_index_of_top_n_items</span>
<span class="p">)</span>

<span class="c1"># Import the utility functions that are provided</span>
<span class="kn">from</span> <span class="nn">util.util</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">check_answer_df</span><span class="p">,</span>
    <span class="n">check_answer_np</span><span class="p">,</span>
    <span class="n">wordnet_pos</span><span class="p">,</span>
    <span class="n">reformat_data</span><span class="p">,</span>
    <span class="n">visualize_word_counts</span><span class="p">,</span>
    <span class="n">add_spacy_doc</span><span class="p">,</span>
    <span class="n">add_spacy_tokens</span><span class="p">,</span>
    <span class="n">add_padded_tensors</span>
<span class="p">)</span>

<span class="c1"># Add tqdm functions to pandas.</span>
<span class="n">tqdm</span><span class="o">.</span><span class="n">pandas</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><a name="answer"></a></p>
</section>
<section id="task-answers">
<h2>Task Answers<a class="headerlink" href="#task-answers" title="Permalink to this headline">#</a></h2>
<p>Click on one of the following links to check answers for the assignments in this tutorial. <strong>Do not check the answers before practicing the tasks.</strong></p>
<ul class="simple">
<li><p><span class="xref myst">Click this for task answers if you open this notebook on your local machine</span></p></li>
<li><p><a class="reference internal" href="answer.html"><span class="doc">Click this for task answers if you view this notebook on a web browser</span></a></p></li>
</ul>
<p><a name="util"></a></p>
</section>
<section id="utility-file">
<h2>Utility File<a class="headerlink" href="#utility-file" title="Permalink to this headline">#</a></h2>
<p>Click on one of the following links to check the provided functions in the utility file for this tutorial.</p>
<ul class="simple">
<li><p><span class="xref myst">Click this for utility functions if you open this notebook on your local machine</span></p></li>
<li><p><a class="reference internal" href="util.html"><span class="doc">Click this for utility functions if you view this notebook on a web browser</span></a></p></li>
</ul>
<p><a name="t3"></a></p>
</section>
<section id="task-3-preprocess-text-data">
<h2>Task 3: Preprocess Text Data<a class="headerlink" href="#task-3-preprocess-text-data" title="Permalink to this headline">#</a></h2>
<p>In this task, we will preprocess the text data from the AG News Dataset. First, we need to load the files.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;train.csv&quot;</span><span class="p">)</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;test.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>For performance reasons, we will only use a small subset of this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;Class Index&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;Class Index&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Class Index</th>
      <th>Title</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>
      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>
      <td>Reuters - Private investment firm Carlyle Grou...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>
      <td>Reuters - Soaring crude prices plus worries\ab...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>
      <td>Reuters - Authorities have halted oil export\f...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>Oil prices soar to all-time record, posing new...</td>
      <td>AFP - Tearaway world oil prices, toppling reco...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>4865</th>
      <td>2</td>
      <td>IOA sets up committee to probe dope scandal</td>
      <td>Athens, Aug 20. (PTI):In a belated damage-cont...</td>
    </tr>
    <tr>
      <th>4866</th>
      <td>2</td>
      <td>FACTBOX-Jonathan Woodgate factbox</td>
      <td>MADRID, Aug 20 (Reuters) - Factbox on England ...</td>
    </tr>
    <tr>
      <th>4867</th>
      <td>2</td>
      <td>British canoe pair lose out</td>
      <td>ATHENS (Reuters) - Slovakian twins Peter and P...</td>
    </tr>
    <tr>
      <th>4895</th>
      <td>2</td>
      <td>U.S. Softball Team Posts Shutout No. 7 (AP)</td>
      <td>AP - Cat Osterman struck out 10 in six innings...</td>
    </tr>
    <tr>
      <th>4896</th>
      <td>2</td>
      <td>Paul Hamm's example</td>
      <td>PAUL HAMM'S fall and rise are what make the Ol...</td>
    </tr>
  </tbody>
</table>
<p>4000 rows × 3 columns</p>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Class Index</th>
      <th>Title</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>Fears for T N pension after talks</td>
      <td>Unions representing workers at Turner   Newall...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>The Race is On: Second Private Team Sets Launc...</td>
      <td>SPACE.com - TORONTO, Canada -- A second\team o...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>
      <td>AP - A company founded by a chemistry research...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>
      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>
      <td>AP - Southern California's smog-fighting agenc...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>473</th>
      <td>3</td>
      <td>New Overtime Rules Take Effect</td>
      <td>New Bush administration rules that scale back ...</td>
    </tr>
    <tr>
      <th>479</th>
      <td>3</td>
      <td>Dollar Holds Gains, Fed Comments Help</td>
      <td>TOKYO (Reuters) - The dollar held on to the p...</td>
    </tr>
    <tr>
      <th>481</th>
      <td>3</td>
      <td>Dark arts of spin evident in phoney war for Abbey</td>
      <td>THE phoney war over the fate of Abbey grinds o...</td>
    </tr>
    <tr>
      <th>482</th>
      <td>3</td>
      <td>Controversial US Overtime Rules Take Effect</td>
      <td>New overtime rules have taken effect in the Un...</td>
    </tr>
    <tr>
      <th>484</th>
      <td>3</td>
      <td>SAS Braathens to cut Gatwick, Geneva flights</td>
      <td>blackhawk writes  quot;SAS Braathens, the Norw...</td>
    </tr>
  </tbody>
</table>
<p>400 rows × 3 columns</p>
</div></div></div>
</div>
<p>As you can see, all the classes are distributed evenly in the train and test data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s2">&quot;Class Index&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(),</span> <span class="n">df_test</span><span class="p">[</span><span class="s2">&quot;Class Index&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Class Index
3    1000
4    1000
2    1000
1    1000
Name: count, dtype: int64
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Class Index
3    100
4    100
2    100
1    100
Name: count, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>To make the data more understandable, we will make the classes more understandable by adding a <code class="docutils literal notranslate"><span class="pre">class</span></code> column from the original <code class="docutils literal notranslate"><span class="pre">Class</span> <span class="pre">Index</span></code> column, containing the category of the news article. To process both the title and news text together, we will combine the <code class="docutils literal notranslate"><span class="pre">Title</span></code> and <code class="docutils literal notranslate"><span class="pre">Description</span></code> columns into one <code class="docutils literal notranslate"><span class="pre">text</span></code> column. We will deal with just the train data until the point where we need the test data again. We provide the <code class="docutils literal notranslate"><span class="pre">reformat_data</span></code> function in the <a class="reference external" href="#util">utility file</a> for this.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_train_reformat</span> <span class="o">=</span> <span class="n">reformat_data</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df_train_reformat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class_idx</th>
      <th>class</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>Business</td>
      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>Business</td>
      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Business</td>
      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>Business</td>
      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>Business</td>
      <td>Oil prices soar to all-time record, posing new...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>4865</th>
      <td>2</td>
      <td>Sports</td>
      <td>IOA sets up committee to probe dope scandal At...</td>
    </tr>
    <tr>
      <th>4866</th>
      <td>2</td>
      <td>Sports</td>
      <td>FACTBOX-Jonathan Woodgate factbox MADRID, Aug ...</td>
    </tr>
    <tr>
      <th>4867</th>
      <td>2</td>
      <td>Sports</td>
      <td>British canoe pair lose out ATHENS (Reuters) -...</td>
    </tr>
    <tr>
      <th>4895</th>
      <td>2</td>
      <td>Sports</td>
      <td>U.S. Softball Team Posts Shutout No. 7 (AP) AP...</td>
    </tr>
    <tr>
      <th>4896</th>
      <td>2</td>
      <td>Sports</td>
      <td>Paul Hamm's example PAUL HAMM'S fall and rise ...</td>
    </tr>
  </tbody>
</table>
<p>4000 rows × 3 columns</p>
</div></div></div>
</div>
<p><a name="t3-1"></a></p>
<section id="tokenization">
<h3>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">#</a></h3>
<p>Tokenization is the process of breaking down a text into individual tokens, which are usually words but can also be phrases or sentences. It helps language models to understand and analyze text data by breaking it down into smaller, more manageable pieces. While it may seem like a trivial task, tokenization can be applied in multiple ways and thus be a complex and challenging task influencing NLP applications.</p>
<p>For example, in languages like English, it is generally straightforward to identify words by using spaces as delimiters. However, there are exceptions, such as contractions like “can’t” and hyphenated words like “self-driving”. And in Dutch, where multiple nouns can be combined into one bigger noun without any delimiter this can be hard. How would you tokenize “hippopotomonstrosesquippedaliofobie”? In other languages, such as Chinese and Japanese, there are no spaces between words, so identifying word boundaries is much more difficult.</p>
<p>To illustrate the use of tokenization, let’s consider the following example, which tokenizes a sample text using the <code class="docutils literal notranslate"><span class="pre">word_tokenize</span></code> function from the NLTK package. That function uses a pre-trained tokenization model for English.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sample text.</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;The quick brown fox jumped over the lazy dog. The cats couldn&#39;t wait to sleep all day.&quot;</span>

<span class="c1"># Tokenize the text.</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># Print the text and the tokens.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original text:&quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tokenized text:&quot;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original text: The quick brown fox jumped over the lazy dog. The cats couldn&#39;t wait to sleep all day.
Tokenized text: [&#39;The&#39;, &#39;quick&#39;, &#39;brown&#39;, &#39;fox&#39;, &#39;jumped&#39;, &#39;over&#39;, &#39;the&#39;, &#39;lazy&#39;, &#39;dog&#39;, &#39;.&#39;, &#39;The&#39;, &#39;cats&#39;, &#39;could&#39;, &quot;n&#39;t&quot;, &#39;wait&#39;, &#39;to&#39;, &#39;sleep&#39;, &#39;all&#39;, &#39;day&#39;, &#39;.&#39;]
</pre></div>
</div>
</div>
</div>
<p><a name="t3-2"></a></p>
</section>
<section id="part-of-speech-tagging">
<h3>Part-of-speech tagging<a class="headerlink" href="#part-of-speech-tagging" title="Permalink to this headline">#</a></h3>
<p>Part-of-speech (POS) tagging is the process of assigning each word in a text corpus with a specific part-of-speech tag based on its context and definition. The tags typically include nouns, verbs, adjectives, adverbs, pronouns, prepositions, conjunctions, interjections, and more. POS tagging can help other NLP tasks disambiguate a token somewhat due to the added context.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pos_tags</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pos_tags</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;The&#39;, &#39;DT&#39;), (&#39;quick&#39;, &#39;JJ&#39;), (&#39;brown&#39;, &#39;NN&#39;), (&#39;fox&#39;, &#39;NN&#39;), (&#39;jumped&#39;, &#39;VBD&#39;), (&#39;over&#39;, &#39;IN&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;lazy&#39;, &#39;JJ&#39;), (&#39;dog&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;), (&#39;The&#39;, &#39;DT&#39;), (&#39;cats&#39;, &#39;NNS&#39;), (&#39;could&#39;, &#39;MD&#39;), (&quot;n&#39;t&quot;, &#39;RB&#39;), (&#39;wait&#39;, &#39;VB&#39;), (&#39;to&#39;, &#39;TO&#39;), (&#39;sleep&#39;, &#39;VB&#39;), (&#39;all&#39;, &#39;DT&#39;), (&#39;day&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;)]
</pre></div>
</div>
</div>
</div>
<p><a name="t3-3"></a></p>
</section>
<section id="stemming-lemmatization">
<h3>Stemming / Lemmatization<a class="headerlink" href="#stemming-lemmatization" title="Permalink to this headline">#</a></h3>
<p>Stemming and lemmatization are two common techniques used in NLP to preprocess and normalize text data. Both techniques involve transforming words into their root form, but they differ in their approach and the level of normalization they provide.</p>
<p>Stemming is a technique that involves reducing words to their base or stem form by removing any affixes or suffixes. For example, the stem of the word “lazily” would be “lazi”. Stemming is a simple and fast technique that can be useful. However, it can also produce inaccurate or incorrect results since it does not consider the context or part of speech of the word.</p>
<p>Lemmatization, on the other hand, is a more sophisticated technique that involves identifying the base or dictionary form of a word, also known as the lemma. Unlike stemming, lemmatization can consider the part of speech (POS) of the word, which can make it more accurate and reliable. With lemmatization, the lemma of the word “lazily” would be “lazy”. Lemmatization can be slower and more complex than stemming but provides a higher level of normalization.</p>
<p>Also, lemmatization requires POS tagging. Since the POS tagging logic in the <code class="docutils literal notranslate"><span class="pre">nltk</span></code> package is different from the <code class="docutils literal notranslate"><span class="pre">wordnet</span></code> package, we provide a function <code class="docutils literal notranslate"><span class="pre">wordnet_pos</span></code> in the <a class="reference external" href="#util">utility file</a> for converting the POS tags. The input of the <code class="docutils literal notranslate"><span class="pre">wordnet_pos</span></code> function is a <code class="docutils literal notranslate"><span class="pre">nltk</span></code> POS tag, and the output of the function is a <code class="docutils literal notranslate"><span class="pre">wordnet</span></code> POS tag.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize the stemmer and lemmatizer.</span>
<span class="n">stemmer</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>

<span class="c1"># Perform stemming and lemmatization seperately on the tokens.</span>
<span class="n">stemmed_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
<span class="n">lemmatized_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">wordnet_pos</span><span class="p">(</span><span class="n">tag</span><span class="p">))</span>
                     <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">nltk</span><span class="o">.</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">)]</span>

<span class="c1"># Print the results.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stemmed text:&quot;</span><span class="p">,</span> <span class="n">stemmed_tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Lemmatized text:&quot;</span><span class="p">,</span> <span class="n">lemmatized_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stemmed text: [&#39;the&#39;, &#39;quick&#39;, &#39;brown&#39;, &#39;fox&#39;, &#39;jump&#39;, &#39;over&#39;, &#39;the&#39;, &#39;lazi&#39;, &#39;dog&#39;, &#39;.&#39;, &#39;the&#39;, &#39;cat&#39;, &#39;could&#39;, &quot;n&#39;t&quot;, &#39;wait&#39;, &#39;to&#39;, &#39;sleep&#39;, &#39;all&#39;, &#39;day&#39;, &#39;.&#39;]
Lemmatized text: [&#39;The&#39;, &#39;quick&#39;, &#39;brown&#39;, &#39;fox&#39;, &#39;jump&#39;, &#39;over&#39;, &#39;the&#39;, &#39;lazy&#39;, &#39;dog&#39;, &#39;.&#39;, &#39;The&#39;, &#39;cat&#39;, &#39;could&#39;, &quot;n&#39;t&quot;, &#39;wait&#39;, &#39;to&#39;, &#39;sleep&#39;, &#39;all&#39;, &#39;day&#39;, &#39;.&#39;]
</pre></div>
</div>
</div>
</div>
<p><a name="t3-4"></a></p>
</section>
<section id="stopword-removal">
<h3>Stopword Removal<a class="headerlink" href="#stopword-removal" title="Permalink to this headline">#</a></h3>
<p>Stopword removal is a common technique used in NLP to preprocess and clean text data by removing words that are considered to be of little or no value in terms of conveying meaning or information. These words are called “stopwords” and they include common words such as “the”, “a”, “an”, “and”, “or”, “but”, and so on.</p>
<p>The purpose of stopword removal in NLP is to improve the accuracy and efficiency of text analysis and processing by reducing the noise and complexity of the data. Stopwords are often used to form grammatical structures in a sentence, but they do not carry much meaning or relevance to the main topic or theme of the text. So by removing these words, we can reduce the dimensionality of the text data, improve the performance of machine learning models, and speed up the processing of text data. NLTK has a predefined list of stopwords for English.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># English stopwords in NLTK.</span>
<span class="n">stopwords_list</span> <span class="o">=</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stopwords_list</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;, &#39;ourselves&#39;, &#39;you&#39;, &quot;you&#39;re&quot;, &quot;you&#39;ve&quot;, &quot;you&#39;ll&quot;, &quot;you&#39;d&quot;, &#39;your&#39;, &#39;yours&#39;, &#39;yourself&#39;, &#39;yourselves&#39;, &#39;he&#39;, &#39;him&#39;, &#39;his&#39;, &#39;himself&#39;, &#39;she&#39;, &quot;she&#39;s&quot;, &#39;her&#39;, &#39;hers&#39;, &#39;herself&#39;, &#39;it&#39;, &quot;it&#39;s&quot;, &#39;its&#39;, &#39;itself&#39;, &#39;they&#39;, &#39;them&#39;, &#39;their&#39;, &#39;theirs&#39;, &#39;themselves&#39;, &#39;what&#39;, &#39;which&#39;, &#39;who&#39;, &#39;whom&#39;, &#39;this&#39;, &#39;that&#39;, &quot;that&#39;ll&quot;, &#39;these&#39;, &#39;those&#39;, &#39;am&#39;, &#39;is&#39;, &#39;are&#39;, &#39;was&#39;, &#39;were&#39;, &#39;be&#39;, &#39;been&#39;, &#39;being&#39;, &#39;have&#39;, &#39;has&#39;, &#39;had&#39;, &#39;having&#39;, &#39;do&#39;, &#39;does&#39;, &#39;did&#39;, &#39;doing&#39;, &#39;a&#39;, &#39;an&#39;, &#39;the&#39;, &#39;and&#39;, &#39;but&#39;, &#39;if&#39;, &#39;or&#39;, &#39;because&#39;, &#39;as&#39;, &#39;until&#39;, &#39;while&#39;, &#39;of&#39;, &#39;at&#39;, &#39;by&#39;, &#39;for&#39;, &#39;with&#39;, &#39;about&#39;, &#39;against&#39;, &#39;between&#39;, &#39;into&#39;, &#39;through&#39;, &#39;during&#39;, &#39;before&#39;, &#39;after&#39;, &#39;above&#39;, &#39;below&#39;, &#39;to&#39;, &#39;from&#39;, &#39;up&#39;, &#39;down&#39;, &#39;in&#39;, &#39;out&#39;, &#39;on&#39;, &#39;off&#39;, &#39;over&#39;, &#39;under&#39;, &#39;again&#39;, &#39;further&#39;, &#39;then&#39;, &#39;once&#39;, &#39;here&#39;, &#39;there&#39;, &#39;when&#39;, &#39;where&#39;, &#39;why&#39;, &#39;how&#39;, &#39;all&#39;, &#39;any&#39;, &#39;both&#39;, &#39;each&#39;, &#39;few&#39;, &#39;more&#39;, &#39;most&#39;, &#39;other&#39;, &#39;some&#39;, &#39;such&#39;, &#39;no&#39;, &#39;nor&#39;, &#39;not&#39;, &#39;only&#39;, &#39;own&#39;, &#39;same&#39;, &#39;so&#39;, &#39;than&#39;, &#39;too&#39;, &#39;very&#39;, &#39;s&#39;, &#39;t&#39;, &#39;can&#39;, &#39;will&#39;, &#39;just&#39;, &#39;don&#39;, &quot;don&#39;t&quot;, &#39;should&#39;, &quot;should&#39;ve&quot;, &#39;now&#39;, &#39;d&#39;, &#39;ll&#39;, &#39;m&#39;, &#39;o&#39;, &#39;re&#39;, &#39;ve&#39;, &#39;y&#39;, &#39;ain&#39;, &#39;aren&#39;, &quot;aren&#39;t&quot;, &#39;couldn&#39;, &quot;couldn&#39;t&quot;, &#39;didn&#39;, &quot;didn&#39;t&quot;, &#39;doesn&#39;, &quot;doesn&#39;t&quot;, &#39;hadn&#39;, &quot;hadn&#39;t&quot;, &#39;hasn&#39;, &quot;hasn&#39;t&quot;, &#39;haven&#39;, &quot;haven&#39;t&quot;, &#39;isn&#39;, &quot;isn&#39;t&quot;, &#39;ma&#39;, &#39;mightn&#39;, &quot;mightn&#39;t&quot;, &#39;mustn&#39;, &quot;mustn&#39;t&quot;, &#39;needn&#39;, &quot;needn&#39;t&quot;, &#39;shan&#39;, &quot;shan&#39;t&quot;, &#39;shouldn&#39;, &quot;shouldn&#39;t&quot;, &#39;wasn&#39;, &quot;wasn&#39;t&quot;, &#39;weren&#39;, &quot;weren&#39;t&quot;, &#39;won&#39;, &quot;won&#39;t&quot;, &#39;wouldn&#39;, &quot;wouldn&#39;t&quot;]
</pre></div>
</div>
</div>
</div>
<p><a name="a3-1"></a></p>
</section>
<section id="assignment-for-task-3-1-tokenization-and-lemmatization">
<h3>Assignment for Task 3.1: Tokenization and Lemmatization<a class="headerlink" href="#assignment-for-task-3-1-tokenization-and-lemmatization" title="Permalink to this headline">#</a></h3>
<p>The first step is to tokenize and lemmatize the sentences. <strong>Your task (which is your assignment) is to write functions to do the following:</strong></p>
<ul class="simple">
<li><p>Since we want to use our text to make a model later on, we need to preprocess it. Add a <code class="docutils literal notranslate"><span class="pre">tokens</span></code> column to the <code class="docutils literal notranslate"><span class="pre">df_train</span></code> dataframe with the text tokenized, then lemmatize those tokens. You must use the POS tags when lemmatizing.</p>
<ul>
<li><p>Hint: Use the <code class="docutils literal notranslate"><span class="pre">pandas.Series.apply</span></code> function with the imported <code class="docutils literal notranslate"><span class="pre">nltk.tokenize.word_tokenize</span></code> function. Recall that you can use the <code class="docutils literal notranslate"><span class="pre">pd.Series.apply?</span></code> syntax in a code cell for more information.</p></li>
<li><p>Hint: use the <code class="docutils literal notranslate"><span class="pre">nltk.stem.WordNetLemmatizer.lemmatize</span></code> function to lemmatize a token. Use the <code class="docutils literal notranslate"><span class="pre">wordnet_pos</span></code> function to obtain the POS tag for the lemmatizer.</p></li>
</ul>
</li>
<li><p>Tokenizing and lemmatizing the entire dataset can take a while too. Use <code class="docutils literal notranslate"><span class="pre">tqdm</span></code> and the <code class="docutils literal notranslate"><span class="pre">pandas.Series.progress_apply</span></code> to show progress bars for the operations.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_and_lemmatize</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tokenize and lemmatize the text in the dataset.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : pandas.DataFrame</span>
<span class="sd">        The dataframe containing at least the &quot;text&quot; column.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pandas.DataFrame</span>
<span class="sd">        The dataframe with the added &quot;tokens&quot; column.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">###################################</span>
    <span class="c1"># Fill in your answer here</span>
    <span class="k">return</span> <span class="kc">None</span>
    <span class="c1">###################################</span>
</pre></div>
</div>
</div>
</div>
<p>Our goal is to have a dataframe that looks like the following. <strong>For simplicity, we only show the top 5 most frequent words.</strong> Your data frame should have more rows.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This part of code will take some time to run.</span>
<span class="n">answer_df_with_tokens</span> <span class="o">=</span> <span class="n">answer_tokenize_and_lemmatize</span><span class="p">(</span><span class="n">df_train_reformat</span><span class="p">)</span>
<span class="n">answer_df_with_tokens</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "adc2f2c5e39741b8924fcfd101159c86", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "ed5c609a458e4c119b914b5145a2de28", "version_major": 2, "version_minor": 0}
</script><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class_idx</th>
      <th>class</th>
      <th>text</th>
      <th>tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>Business</td>
      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>
      <td>[Wall, St., Bears, Claw, Back, Into, the, Blac...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>Business</td>
      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>
      <td>[Carlyle, Looks, Toward, Commercial, Aerospace...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Business</td>
      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>
      <td>[Oil, and, Economy, Cloud, Stocks, ', Outlook,...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>Business</td>
      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>
      <td>[Iraq, Halts, Oil, Exports, from, Main, Southe...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>Business</td>
      <td>Oil prices soar to all-time record, posing new...</td>
      <td>[Oil, price, soar, to, all-time, record, ,, po...</td>
    </tr>
    <tr>
      <th>78</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>'Madden,' 'ESPN' Football Score in Different W...</td>
      <td>['Madden, ,, ', 'ESPN, ', Football, Score, in,...</td>
    </tr>
    <tr>
      <th>79</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>Group to Propose New High-Speed Wireless Forma...</td>
      <td>[Group, to, Propose, New, High-Speed, Wireless...</td>
    </tr>
    <tr>
      <th>80</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>AOL to Sell Cheap PCs to Minorities and Senior...</td>
      <td>[AOL, to, Sell, Cheap, PCs, to, Minorities, an...</td>
    </tr>
    <tr>
      <th>81</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>Companies Approve New High-Capacity Disc Forma...</td>
      <td>[Companies, Approve, New, High-Capacity, Disc,...</td>
    </tr>
    <tr>
      <th>82</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>Missing June Deals Slow to Return for Software...</td>
      <td>[Missing, June, Deals, Slow, to, Return, for, ...</td>
    </tr>
    <tr>
      <th>448</th>
      <td>2</td>
      <td>Sports</td>
      <td>Phelps, Thorpe Advance in 200 Freestyle (AP) A...</td>
      <td>[Phelps, ,, Thorpe, Advance, in, 200, Freestyl...</td>
    </tr>
    <tr>
      <th>449</th>
      <td>2</td>
      <td>Sports</td>
      <td>Reds Knock Padres Out of Wild-Card Lead (AP) A...</td>
      <td>[Reds, Knock, Padres, Out, of, Wild-Card, Lead...</td>
    </tr>
    <tr>
      <th>450</th>
      <td>2</td>
      <td>Sports</td>
      <td>Dreaming done, NBA stars awaken to harsh Olymp...</td>
      <td>[Dreaming, do, ,, NBA, star, awaken, to, harsh...</td>
    </tr>
    <tr>
      <th>451</th>
      <td>2</td>
      <td>Sports</td>
      <td>Indians Beat Twins 7-1, Nearing AL Lead (AP) A...</td>
      <td>[Indians, Beat, Twins, 7-1, ,, Nearing, AL, Le...</td>
    </tr>
    <tr>
      <th>452</th>
      <td>2</td>
      <td>Sports</td>
      <td>Galaxy, Crew Play to 0-0 Tie (AP) AP - Kevin H...</td>
      <td>[Galaxy, ,, Crew, Play, to, 0-0, Tie, (, AP, )...</td>
    </tr>
    <tr>
      <th>492</th>
      <td>1</td>
      <td>World</td>
      <td>Venezuelans Vote Early in Referendum on Chavez...</td>
      <td>[Venezuelans, Vote, Early, in, Referendum, on,...</td>
    </tr>
    <tr>
      <th>493</th>
      <td>1</td>
      <td>World</td>
      <td>S.Koreans Clash with Police on Iraq Troop Disp...</td>
      <td>[S.Koreans, Clash, with, Police, on, Iraq, Tro...</td>
    </tr>
    <tr>
      <th>494</th>
      <td>1</td>
      <td>World</td>
      <td>Palestinians in Israeli Jails Start Hunger Str...</td>
      <td>[Palestinians, in, Israeli, Jails, Start, Hung...</td>
    </tr>
    <tr>
      <th>495</th>
      <td>1</td>
      <td>World</td>
      <td>Seven Georgian soldiers wounded as South Osset...</td>
      <td>[Seven, Georgian, soldier, wound, a, South, Os...</td>
    </tr>
    <tr>
      <th>496</th>
      <td>1</td>
      <td>World</td>
      <td>Rwandan Troops Arrive in Darfur (AP) AP - Doze...</td>
      <td>[Rwandan, Troops, Arrive, in, Darfur, (, AP, )...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The code below tests if the output of your function matches the expected output.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_with_tokens</span> <span class="o">=</span> <span class="n">tokenize_and_lemmatize</span><span class="p">(</span><span class="n">df_train_reformat</span><span class="p">)</span>
<span class="n">check_answer_df</span><span class="p">(</span><span class="n">df_with_tokens</span><span class="p">,</span> <span class="n">answer_df_with_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test case 1 failed.

Your output is:
None

Expected output is:
      class_idx     class                                               text  \
0             3  Business  Wall St. Bears Claw Back Into the Black (Reute...   
1             3  Business  Carlyle Looks Toward Commercial Aerospace (Reu...   
2             3  Business  Oil and Economy Cloud Stocks&#39; Outlook (Reuters...   
3             3  Business  Iraq Halts Oil Exports from Main Southern Pipe...   
4             3  Business  Oil prices soar to all-time record, posing new...   
...         ...       ...                                                ...   
4865          2    Sports  IOA sets up committee to probe dope scandal At...   
4866          2    Sports  FACTBOX-Jonathan Woodgate factbox MADRID, Aug ...   
4867          2    Sports  British canoe pair lose out ATHENS (Reuters) -...   
4895          2    Sports  U.S. Softball Team Posts Shutout No. 7 (AP) AP...   
4896          2    Sports  Paul Hamm&#39;s example PAUL HAMM&#39;S fall and rise ...   

                                                 tokens  
0     [Wall, St., Bears, Claw, Back, Into, the, Blac...  
1     [Carlyle, Looks, Toward, Commercial, Aerospace...  
2     [Oil, and, Economy, Cloud, Stocks, &#39;, Outlook,...  
3     [Iraq, Halts, Oil, Exports, from, Main, Southe...  
4     [Oil, price, soar, to, all-time, record, ,, po...  
...                                                 ...  
4865  [IOA, set, up, committee, to, probe, dope, sca...  
4866  [FACTBOX-Jonathan, Woodgate, factbox, MADRID, ...  
4867  [British, canoe, pair, lose, out, ATHENS, (, R...  
4895  [U.S., Softball, Team, Posts, Shutout, No, ., ...  
4896  [Paul, Hamm, &#39;s, example, PAUL, HAMM, &#39;S, fall...  

[4000 rows x 4 columns]
</pre></div>
</div>
</div>
</div>
<p><a name="a3-2"></a></p>
</section>
<section id="assignment-for-task-3-2-word-counts">
<h3>Assignment for Task 3.2: Word Counts<a class="headerlink" href="#assignment-for-task-3-2-word-counts" title="Permalink to this headline">#</a></h3>
<p>To see what the most used words per class are, create a new, seperate dataframe with token counts.</p>
<ul class="simple">
<li><p>Hint: use the <code class="docutils literal notranslate"><span class="pre">pandas.Series.apply</span></code> and <code class="docutils literal notranslate"><span class="pre">str.isalpha()</span></code> functions to filter out non-alphabetical tokens.</p></li>
<li><p>Hint: use the <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame.explode</span></code> to create one row per class and token.</p></li>
<li><p>Hint: use <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame.groupby</span></code> with <code class="docutils literal notranslate"><span class="pre">.size()</span></code> afterwards or <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame.pivot_table</span></code> with size as the <code class="docutils literal notranslate"><span class="pre">aggfunc</span></code> to obtain the occurences per class.</p></li>
<li><p>Hint: use the <code class="docutils literal notranslate"><span class="pre">pandas.Series.reset_index</span></code> function to obtain a dataframe with [class, tokens, count] as the columns.</p></li>
<li><p>Hint: use the <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame.sort_values</span></code> function for sorting a dataframe.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_word_counts</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">token_col</span><span class="o">=</span><span class="s2">&quot;tokens&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate dataframes with the word counts for each class in the data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : pandas.DataFrame</span>
<span class="sd">        The dataframe containing at least the &quot;class&quot; and &quot;tokens&quot; columns.</span>
<span class="sd">    token_col : str</span>
<span class="sd">        Name of the column that stores the tokens.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pandas.DataFrame</span>
<span class="sd">        There should be three columns in this dataframe.</span>
<span class="sd">        The &quot;class&quot; column shows the document class.</span>
<span class="sd">        The &quot;tokens&quot; column means tokens in the document class.</span>
<span class="sd">        The &quot;count&quot; column means the number of appearances of each token in the class.</span>
<span class="sd">        The dataframe should be sorted by the &quot;class&quot; and &quot;count&quot; columns.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">###################################</span>
    <span class="c1"># Fill in your answer here</span>
    <span class="k">return</span> <span class="kc">None</span>
    <span class="c1">###################################</span>
</pre></div>
</div>
</div>
</div>
<p>Our goal is to have a dictionary of dataframes (one per class) that look like the following. <strong>For simplicity, we only show the top 5 most frequent words.</strong> Your data frame should have more rows.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">answer_word_counts</span> <span class="o">=</span> <span class="n">answer_get_word_counts</span><span class="p">(</span><span class="n">answer_df_with_tokens</span><span class="p">,</span> <span class="n">token_col</span><span class="o">=</span><span class="s2">&quot;tokens&quot;</span><span class="p">)</span>
<span class="n">answer_word_counts</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class</th>
      <th>tokens</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3541</th>
      <td>Business</td>
      <td>the</td>
      <td>1460</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Business</td>
      <td>a</td>
      <td>1217</td>
    </tr>
    <tr>
      <th>3587</th>
      <td>Business</td>
      <td>to</td>
      <td>923</td>
    </tr>
    <tr>
      <th>1743</th>
      <td>Business</td>
      <td>in</td>
      <td>776</td>
    </tr>
    <tr>
      <th>2424</th>
      <td>Business</td>
      <td>on</td>
      <td>687</td>
    </tr>
    <tr>
      <th>8929</th>
      <td>Sci/Tech</td>
      <td>the</td>
      <td>1660</td>
    </tr>
    <tr>
      <th>3961</th>
      <td>Sci/Tech</td>
      <td>a</td>
      <td>1121</td>
    </tr>
    <tr>
      <th>8997</th>
      <td>Sci/Tech</td>
      <td>to</td>
      <td>1074</td>
    </tr>
    <tr>
      <th>7319</th>
      <td>Sci/Tech</td>
      <td>of</td>
      <td>939</td>
    </tr>
    <tr>
      <th>4423</th>
      <td>Sci/Tech</td>
      <td>be</td>
      <td>744</td>
    </tr>
    <tr>
      <th>13816</th>
      <td>Sports</td>
      <td>the</td>
      <td>2280</td>
    </tr>
    <tr>
      <th>9565</th>
      <td>Sports</td>
      <td>a</td>
      <td>978</td>
    </tr>
    <tr>
      <th>11631</th>
      <td>Sports</td>
      <td>in</td>
      <td>815</td>
    </tr>
    <tr>
      <th>13888</th>
      <td>Sports</td>
      <td>to</td>
      <td>793</td>
    </tr>
    <tr>
      <th>12471</th>
      <td>Sports</td>
      <td>of</td>
      <td>714</td>
    </tr>
    <tr>
      <th>18749</th>
      <td>World</td>
      <td>the</td>
      <td>1653</td>
    </tr>
    <tr>
      <th>14384</th>
      <td>World</td>
      <td>a</td>
      <td>1241</td>
    </tr>
    <tr>
      <th>18808</th>
      <td>World</td>
      <td>to</td>
      <td>1172</td>
    </tr>
    <tr>
      <th>16474</th>
      <td>World</td>
      <td>in</td>
      <td>1164</td>
    </tr>
    <tr>
      <th>17302</th>
      <td>World</td>
      <td>of</td>
      <td>908</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The code below tests if the output of your function matches the expected output.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_counts</span> <span class="o">=</span> <span class="n">get_word_counts</span><span class="p">(</span><span class="n">df_with_tokens</span><span class="p">,</span> <span class="n">token_col</span><span class="o">=</span><span class="s2">&quot;tokens&quot;</span><span class="p">)</span>
<span class="n">check_answer_df</span><span class="p">(</span><span class="n">answer_word_counts</span><span class="p">,</span> <span class="n">word_counts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test case 1 failed.

Your output is:
          class      tokens  count
3541   Business         the   1460
0      Business           a   1217
3587   Business          to    923
1743   Business          in    776
2424   Business          on    687
...         ...         ...    ...
19259     World      zalmay      1
19261     World     zeitoun      1
19262     World        zesn      1
19263     World         zim      1
19265     World  zimbabwean      1

[19268 rows x 3 columns]

Expected output is:
None
</pre></div>
</div>
</div>
</div>
<p>We provide the <code class="docutils literal notranslate"><span class="pre">visualize_word_counts</span></code> function in the <a class="reference external" href="#util">utility file</a> to visualize the word counts that you just computed using the <a class="reference external" href="https://amueller.github.io/word_cloud/auto_examples/simple.html#sphx-glr-auto-examples-simple-py">wordcloud</a> package.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_word_counts</span><span class="p">(</span><span class="n">answer_word_counts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/tutorial-text-data_58_0.png" src="../_images/tutorial-text-data_58_0.png" />
<img alt="../_images/tutorial-text-data_58_1.png" src="../_images/tutorial-text-data_58_1.png" />
<img alt="../_images/tutorial-text-data_58_2.png" src="../_images/tutorial-text-data_58_2.png" />
<img alt="../_images/tutorial-text-data_58_3.png" src="../_images/tutorial-text-data_58_3.png" />
</div>
</div>
<p><a name="a3-3"></a></p>
</section>
<section id="assignment-for-task-3-3-stop-words-removal">
<h3>Assignment for Task 3.3: Stop Words Removal<a class="headerlink" href="#assignment-for-task-3-3-stop-words-removal" title="Permalink to this headline">#</a></h3>
<p>The stop words make it difficult for us to identify representative words for each class. Let’s display the word counts using the data without stop words. But we need to remove the stop words first. <strong>Your task (which is your assignment) is to write functions to do the following:</strong></p>
<ul class="simple">
<li><p>Remove the stopwords from the tokens column in the dataframe.</p>
<ul>
<li><p>Hint: use the <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame.isin</span></code> function.</p></li>
<li><p>Hint: use the <code class="docutils literal notranslate"><span class="pre">stopwords_list</span></code> variable to help you check if a token is a stop word.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">remove_stopwords</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Remove stopwords from the tokens.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : pandas.DataFrame</span>
<span class="sd">        There should be three columns in this dataframe.</span>
<span class="sd">        The &quot;class&quot; column shows the document class.</span>
<span class="sd">        The &quot;tokens&quot; column means tokens in the document class.</span>
<span class="sd">        The &quot;count&quot; column means the number of appearances of each token in the class.</span>
<span class="sd">        The dataframe should be sorted by the &quot;class&quot; and &quot;count&quot; columns.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pandas.DataFrame</span>
<span class="sd">        The dataframe with the stopwords rows removed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">###################################</span>
    <span class="c1"># Fill in your answer here</span>
    <span class="k">return</span> <span class="kc">None</span>
    <span class="c1">###################################</span>
</pre></div>
</div>
</div>
</div>
<p>Our goal is to have a dictionary of dataframes (one per class) that look like the following. <strong>For simplicity, we only show the top 5 most frequent words.</strong> Your data frame should have more rows.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">answer_word_counts_no_stopword</span> <span class="o">=</span> <span class="n">answer_remove_stopwords</span><span class="p">(</span><span class="n">answer_word_counts</span><span class="p">)</span>
<span class="n">answer_word_counts_no_stopword</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class</th>
      <th>tokens</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2948</th>
      <td>Business</td>
      <td>reuters</td>
      <td>483</td>
    </tr>
    <tr>
      <th>2351</th>
      <td>Business</td>
      <td>new</td>
      <td>297</td>
    </tr>
    <tr>
      <th>2698</th>
      <td>Business</td>
      <td>price</td>
      <td>281</td>
    </tr>
    <tr>
      <th>3041</th>
      <td>Business</td>
      <td>say</td>
      <td>281</td>
    </tr>
    <tr>
      <th>2419</th>
      <td>Business</td>
      <td>oil</td>
      <td>278</td>
    </tr>
    <tr>
      <th>4209</th>
      <td>Sci/Tech</td>
      <td>ap</td>
      <td>236</td>
    </tr>
    <tr>
      <th>7228</th>
      <td>Sci/Tech</td>
      <td>new</td>
      <td>224</td>
    </tr>
    <tr>
      <th>8237</th>
      <td>Sci/Tech</td>
      <td>say</td>
      <td>186</td>
    </tr>
    <tr>
      <th>4952</th>
      <td>Sci/Tech</td>
      <td>company</td>
      <td>137</td>
    </tr>
    <tr>
      <th>7031</th>
      <td>Sci/Tech</td>
      <td>microsoft</td>
      <td>121</td>
    </tr>
    <tr>
      <th>9817</th>
      <td>Sports</td>
      <td>athens</td>
      <td>354</td>
    </tr>
    <tr>
      <th>9747</th>
      <td>Sports</td>
      <td>ap</td>
      <td>315</td>
    </tr>
    <tr>
      <th>12497</th>
      <td>Sports</td>
      <td>olympic</td>
      <td>244</td>
    </tr>
    <tr>
      <th>14281</th>
      <td>Sports</td>
      <td>win</td>
      <td>223</td>
    </tr>
    <tr>
      <th>11335</th>
      <td>Sports</td>
      <td>gold</td>
      <td>208</td>
    </tr>
    <tr>
      <th>18159</th>
      <td>World</td>
      <td>say</td>
      <td>305</td>
    </tr>
    <tr>
      <th>18012</th>
      <td>World</td>
      <td>reuters</td>
      <td>276</td>
    </tr>
    <tr>
      <th>14598</th>
      <td>World</td>
      <td>ap</td>
      <td>261</td>
    </tr>
    <tr>
      <th>17194</th>
      <td>World</td>
      <td>najaf</td>
      <td>154</td>
    </tr>
    <tr>
      <th>14474</th>
      <td>World</td>
      <td>afp</td>
      <td>149</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The code below tests if the output of your function matches the expected output.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_counts_no_stopword</span> <span class="o">=</span> <span class="n">remove_stopwords</span><span class="p">(</span><span class="n">word_counts</span><span class="p">)</span>
<span class="n">check_answer_df</span><span class="p">(</span><span class="n">word_counts_no_stopword</span><span class="p">,</span> <span class="n">answer_word_counts_no_stopword</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test case 1 failed.

Your output is:
None

Expected output is:
          class      tokens  count
2948   Business     reuters    483
2351   Business         new    297
2698   Business       price    281
3041   Business         say    281
2419   Business         oil    278
...         ...         ...    ...
19259     World      zalmay      1
19261     World     zeitoun      1
19262     World        zesn      1
19263     World         zim      1
19265     World  zimbabwean      1

[18787 rows x 3 columns]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_word_counts</span><span class="p">(</span><span class="n">answer_word_counts_no_stopword</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/tutorial-text-data_67_0.png" src="../_images/tutorial-text-data_67_0.png" />
<img alt="../_images/tutorial-text-data_67_1.png" src="../_images/tutorial-text-data_67_1.png" />
<img alt="../_images/tutorial-text-data_67_2.png" src="../_images/tutorial-text-data_67_2.png" />
<img alt="../_images/tutorial-text-data_67_3.png" src="../_images/tutorial-text-data_67_3.png" />
</div>
</div>
<p><a name="spacy"></a></p>
</section>
<section id="another-option-spacy">
<h3>Another Option: spaCy<a class="headerlink" href="#another-option-spacy" title="Permalink to this headline">#</a></h3>
<p>spaCy is another library used to perform various NLP tasks like tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and much more. It provides pre-trained models for different languages and domains, which can be used as-is but also can be fine-tuned on a specific task or domain.</p>
<p>In an object-oriented way, spaCy can be thought of as a collection of classes and objects that work together to perform NLP tasks. Some of the important functions and classes in spaCy include:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">nlp</span></code>: The core function that provides the main functionality of spaCy. It is used to process text and create a <code class="docutils literal notranslate"><span class="pre">Doc</span></code> object.</p></li>
<li><p><a class="reference external" href="https://spacy.io/api/doc"><code class="docutils literal notranslate"><span class="pre">Doc</span></code></a>: A container for accessing linguistic annotations like tokens, part-of-speech tags, named entities, and dependency parse information. It is created by the <code class="docutils literal notranslate"><span class="pre">nlp</span></code> function and represents a processed document.</p></li>
<li><p><a class="reference external" href="https://spacy.io/api/token"><code class="docutils literal notranslate"><span class="pre">Token</span></code></a>: An object representing a single token in a <code class="docutils literal notranslate"><span class="pre">Doc</span></code> object. It contains information like the token text, part-of-speech tag, lemma, embedding, and much more.</p></li>
</ul>
<p>When a text is processed by spaCy, it is first passed to the <code class="docutils literal notranslate"><span class="pre">nlp</span></code> function, which uses the loaded model to tokenize the text and applies various linguistic annotations like part-of-speech tagging, named entity recognition, and dependency parsing in the background. The resulting annotations are stored in a <code class="docutils literal notranslate"><span class="pre">Doc</span></code> object, which can be accessed and manipulated using various methods and attributes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the small English model in spaCy.</span>
<span class="c1"># Disable Named Entity Recognition and the parser in the model pipeline since we&#39;re not using it.</span>
<span class="c1"># Check the following website for the spaCy NLP pipeline:</span>
<span class="c1"># - https://spacy.io/usage/processing-pipelines</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;parser&quot;</span><span class="p">,</span> <span class="s2">&quot;ner&quot;</span><span class="p">])</span>

<span class="c1"># Process the text using spaCy.</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># This becomes a spaCy Doc object, which prints nicely as the original string.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The quick brown fox jumped over the lazy dog. The cats couldn&#39;t wait to sleep all day.
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Doc</span></code> object can be iterated over to access each <code class="docutils literal notranslate"><span class="pre">Token</span></code> object in the document. We can also directly access multiple attributes of the <code class="docutils literal notranslate"><span class="pre">Token</span></code> objects. For example, we can directly access the lemma of the token with <code class="docutils literal notranslate"><span class="pre">Token.lemma_</span></code> and check if a token is a stop word with <code class="docutils literal notranslate"><span class="pre">Token.is_stop</span></code>. To make it easy to see them, we put them in a data frame.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spacy_doc_attributes</span> <span class="o">=</span> <span class="p">[(</span><span class="n">token</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">is_stop</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">spacy_doc_attributes</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;lemma&quot;</span><span class="p">,</span> <span class="s2">&quot;is_stopword&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>token</th>
      <th>lemma</th>
      <th>is_stopword</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>The</td>
      <td>the</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>quick</td>
      <td>quick</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>brown</td>
      <td>brown</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>fox</td>
      <td>fox</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>jumped</td>
      <td>jump</td>
      <td>False</td>
    </tr>
    <tr>
      <th>5</th>
      <td>over</td>
      <td>over</td>
      <td>True</td>
    </tr>
    <tr>
      <th>6</th>
      <td>the</td>
      <td>the</td>
      <td>True</td>
    </tr>
    <tr>
      <th>7</th>
      <td>lazy</td>
      <td>lazy</td>
      <td>False</td>
    </tr>
    <tr>
      <th>8</th>
      <td>dog</td>
      <td>dog</td>
      <td>False</td>
    </tr>
    <tr>
      <th>9</th>
      <td>.</td>
      <td>.</td>
      <td>False</td>
    </tr>
    <tr>
      <th>10</th>
      <td>The</td>
      <td>the</td>
      <td>True</td>
    </tr>
    <tr>
      <th>11</th>
      <td>cats</td>
      <td>cat</td>
      <td>False</td>
    </tr>
    <tr>
      <th>12</th>
      <td>could</td>
      <td>could</td>
      <td>True</td>
    </tr>
    <tr>
      <th>13</th>
      <td>n't</td>
      <td>not</td>
      <td>True</td>
    </tr>
    <tr>
      <th>14</th>
      <td>wait</td>
      <td>wait</td>
      <td>False</td>
    </tr>
    <tr>
      <th>15</th>
      <td>to</td>
      <td>to</td>
      <td>True</td>
    </tr>
    <tr>
      <th>16</th>
      <td>sleep</td>
      <td>sleep</td>
      <td>False</td>
    </tr>
    <tr>
      <th>17</th>
      <td>all</td>
      <td>all</td>
      <td>True</td>
    </tr>
    <tr>
      <th>18</th>
      <td>day</td>
      <td>day</td>
      <td>False</td>
    </tr>
    <tr>
      <th>19</th>
      <td>.</td>
      <td>.</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The above example only deals with one sentence. Now we need to deal with all the sentences in all the classes. We provide a <code class="docutils literal notranslate"><span class="pre">add_spacy_doc</span></code> function in the <a class="reference external" href="#util">utility file</a> to add a column with a <code class="docutils literal notranslate"><span class="pre">Doc</span></code> representation of the <code class="docutils literal notranslate"><span class="pre">text</span></code> column to the dataframe. Now we can add the spaCy tokens using the above function. This step will take some time since it needs to process all the sentences. So we added a progress bar using the <code class="docutils literal notranslate"><span class="pre">tqdm</span></code> package.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_with_nltk_tokens_and_spacy_doc</span> <span class="o">=</span> <span class="n">add_spacy_doc</span><span class="p">(</span><span class="n">answer_df_with_tokens</span><span class="p">,</span> <span class="n">nlp</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df_with_nltk_tokens_and_spacy_doc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "c03ada6ea4e24483805124f8a00b7df1", "version_major": 2, "version_minor": 0}
</script><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class_idx</th>
      <th>class</th>
      <th>text</th>
      <th>tokens</th>
      <th>doc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>Business</td>
      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>
      <td>[Wall, St., Bears, Claw, Back, Into, the, Blac...</td>
      <td>(Wall, St., Bears, Claw, Back, Into, the, Blac...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>Business</td>
      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>
      <td>[Carlyle, Looks, Toward, Commercial, Aerospace...</td>
      <td>(Carlyle, Looks, Toward, Commercial, Aerospace...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Business</td>
      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>
      <td>[Oil, and, Economy, Cloud, Stocks, ', Outlook,...</td>
      <td>(Oil, and, Economy, Cloud, Stocks, ', Outlook,...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>Business</td>
      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>
      <td>[Iraq, Halts, Oil, Exports, from, Main, Southe...</td>
      <td>(Iraq, Halts, Oil, Exports, from, Main, Southe...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>Business</td>
      <td>Oil prices soar to all-time record, posing new...</td>
      <td>[Oil, price, soar, to, all-time, record, ,, po...</td>
      <td>(Oil, prices, soar, to, all, -, time, record, ...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>4865</th>
      <td>2</td>
      <td>Sports</td>
      <td>IOA sets up committee to probe dope scandal At...</td>
      <td>[IOA, set, up, committee, to, probe, dope, sca...</td>
      <td>(IOA, sets, up, committee, to, probe, dope, sc...</td>
    </tr>
    <tr>
      <th>4866</th>
      <td>2</td>
      <td>Sports</td>
      <td>FACTBOX-Jonathan Woodgate factbox MADRID, Aug ...</td>
      <td>[FACTBOX-Jonathan, Woodgate, factbox, MADRID, ...</td>
      <td>(FACTBOX, -, Jonathan, Woodgate, factbox, MADR...</td>
    </tr>
    <tr>
      <th>4867</th>
      <td>2</td>
      <td>Sports</td>
      <td>British canoe pair lose out ATHENS (Reuters) -...</td>
      <td>[British, canoe, pair, lose, out, ATHENS, (, R...</td>
      <td>(British, canoe, pair, lose, out, ATHENS, (, R...</td>
    </tr>
    <tr>
      <th>4895</th>
      <td>2</td>
      <td>Sports</td>
      <td>U.S. Softball Team Posts Shutout No. 7 (AP) AP...</td>
      <td>[U.S., Softball, Team, Posts, Shutout, No, ., ...</td>
      <td>(U.S., Softball, Team, Posts, Shutout, No, ., ...</td>
    </tr>
    <tr>
      <th>4896</th>
      <td>2</td>
      <td>Sports</td>
      <td>Paul Hamm's example PAUL HAMM'S fall and rise ...</td>
      <td>[Paul, Hamm, 's, example, PAUL, HAMM, 'S, fall...</td>
      <td>(Paul, Hamm, 's, example, PAUL, HAMM, 'S, fall...</td>
    </tr>
  </tbody>
</table>
<p>4000 rows × 5 columns</p>
</div></div></div>
</div>
<p>We also provide a <code class="docutils literal notranslate"><span class="pre">add_spacy_tokens</span></code> function in the <a class="reference external" href="#util">utility file</a> to add the spacy tokens to our original dataframe. We can run the code below to add the spacy tokens.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_with_nltk_tokens_and_spacy_tokens</span> <span class="o">=</span> <span class="n">add_spacy_tokens</span><span class="p">(</span><span class="n">df_with_nltk_tokens_and_spacy_doc</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df_with_nltk_tokens_and_spacy_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class_idx</th>
      <th>class</th>
      <th>text</th>
      <th>tokens</th>
      <th>doc</th>
      <th>spacy_tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>Business</td>
      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>
      <td>[Wall, St., Bears, Claw, Back, Into, the, Blac...</td>
      <td>(Wall, St., Bears, Claw, Back, Into, the, Blac...</td>
      <td>[Wall, Bears, Claw, Black, Reuters, Reuters, s...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>Business</td>
      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>
      <td>[Carlyle, Looks, Toward, Commercial, Aerospace...</td>
      <td>(Carlyle, Looks, Toward, Commercial, Aerospace...</td>
      <td>[Carlyle, look, Commercial, Aerospace, Reuters...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Business</td>
      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>
      <td>[Oil, and, Economy, Cloud, Stocks, ', Outlook,...</td>
      <td>(Oil, and, Economy, Cloud, Stocks, ', Outlook,...</td>
      <td>[oil, Economy, Cloud, Stocks, Outlook, Reuters...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>Business</td>
      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>
      <td>[Iraq, Halts, Oil, Exports, from, Main, Southe...</td>
      <td>(Iraq, Halts, Oil, Exports, from, Main, Southe...</td>
      <td>[Iraq, Halts, Oil, Exports, Main, Southern, Pi...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>Business</td>
      <td>Oil prices soar to all-time record, posing new...</td>
      <td>[Oil, price, soar, to, all-time, record, ,, po...</td>
      <td>(Oil, prices, soar, to, all, -, time, record, ...</td>
      <td>[oil, price, soar, time, record, pose, new, me...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>4865</th>
      <td>2</td>
      <td>Sports</td>
      <td>IOA sets up committee to probe dope scandal At...</td>
      <td>[IOA, set, up, committee, to, probe, dope, sca...</td>
      <td>(IOA, sets, up, committee, to, probe, dope, sc...</td>
      <td>[IOA, set, committee, probe, dope, scandal, At...</td>
    </tr>
    <tr>
      <th>4866</th>
      <td>2</td>
      <td>Sports</td>
      <td>FACTBOX-Jonathan Woodgate factbox MADRID, Aug ...</td>
      <td>[FACTBOX-Jonathan, Woodgate, factbox, MADRID, ...</td>
      <td>(FACTBOX, -, Jonathan, Woodgate, factbox, MADR...</td>
      <td>[FACTBOX, Jonathan, Woodgate, factbox, MADRID,...</td>
    </tr>
    <tr>
      <th>4867</th>
      <td>2</td>
      <td>Sports</td>
      <td>British canoe pair lose out ATHENS (Reuters) -...</td>
      <td>[British, canoe, pair, lose, out, ATHENS, (, R...</td>
      <td>(British, canoe, pair, lose, out, ATHENS, (, R...</td>
      <td>[british, canoe, pair, lose, ATHENS, Reuters, ...</td>
    </tr>
    <tr>
      <th>4895</th>
      <td>2</td>
      <td>Sports</td>
      <td>U.S. Softball Team Posts Shutout No. 7 (AP) AP...</td>
      <td>[U.S., Softball, Team, Posts, Shutout, No, ., ...</td>
      <td>(U.S., Softball, Team, Posts, Shutout, No, ., ...</td>
      <td>[Softball, Team, Posts, Shutout, AP, AP, Cat, ...</td>
    </tr>
    <tr>
      <th>4896</th>
      <td>2</td>
      <td>Sports</td>
      <td>Paul Hamm's example PAUL HAMM'S fall and rise ...</td>
      <td>[Paul, Hamm, 's, example, PAUL, HAMM, 'S, fall...</td>
      <td>(Paul, Hamm, 's, example, PAUL, HAMM, 'S, fall...</td>
      <td>[Paul, Hamm, example, PAUL, HAMM, fall, rise, ...</td>
    </tr>
  </tbody>
</table>
<p>4000 rows × 6 columns</p>
</div></div></div>
</div>
<p>Now we can use the function that we wrote before to get the word count from the spacy tokens.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spacy_word_counts</span> <span class="o">=</span> <span class="n">answer_get_word_counts</span><span class="p">(</span><span class="n">df_with_nltk_tokens_and_spacy_tokens</span><span class="p">,</span> <span class="n">token_col</span><span class="o">=</span><span class="s2">&quot;spacy_tokens&quot;</span><span class="p">)</span>
<span class="n">spacy_word_counts</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;class&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class</th>
      <th>spacy_tokens</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2793</th>
      <td>Business</td>
      <td>reuters</td>
      <td>483</td>
    </tr>
    <tr>
      <th>2563</th>
      <td>Business</td>
      <td>price</td>
      <td>317</td>
    </tr>
    <tr>
      <th>2240</th>
      <td>Business</td>
      <td>new</td>
      <td>298</td>
    </tr>
    <tr>
      <th>2880</th>
      <td>Business</td>
      <td>say</td>
      <td>287</td>
    </tr>
    <tr>
      <th>2299</th>
      <td>Business</td>
      <td>oil</td>
      <td>281</td>
    </tr>
    <tr>
      <th>3931</th>
      <td>Sci/Tech</td>
      <td>ap</td>
      <td>236</td>
    </tr>
    <tr>
      <th>6774</th>
      <td>Sci/Tech</td>
      <td>new</td>
      <td>226</td>
    </tr>
    <tr>
      <th>7739</th>
      <td>Sci/Tech</td>
      <td>say</td>
      <td>161</td>
    </tr>
    <tr>
      <th>4625</th>
      <td>Sci/Tech</td>
      <td>company</td>
      <td>140</td>
    </tr>
    <tr>
      <th>6584</th>
      <td>Sci/Tech</td>
      <td>microsoft</td>
      <td>124</td>
    </tr>
    <tr>
      <th>9178</th>
      <td>Sports</td>
      <td>athens</td>
      <td>356</td>
    </tr>
    <tr>
      <th>9112</th>
      <td>Sports</td>
      <td>ap</td>
      <td>315</td>
    </tr>
    <tr>
      <th>11775</th>
      <td>Sports</td>
      <td>olympic</td>
      <td>262</td>
    </tr>
    <tr>
      <th>13447</th>
      <td>Sports</td>
      <td>win</td>
      <td>233</td>
    </tr>
    <tr>
      <th>10642</th>
      <td>Sports</td>
      <td>gold</td>
      <td>212</td>
    </tr>
    <tr>
      <th>17110</th>
      <td>World</td>
      <td>say</td>
      <td>294</td>
    </tr>
    <tr>
      <th>16967</th>
      <td>World</td>
      <td>reuters</td>
      <td>276</td>
    </tr>
    <tr>
      <th>13728</th>
      <td>World</td>
      <td>ap</td>
      <td>261</td>
    </tr>
    <tr>
      <th>16185</th>
      <td>World</td>
      <td>najaf</td>
      <td>157</td>
    </tr>
    <tr>
      <th>13622</th>
      <td>World</td>
      <td>afp</td>
      <td>150</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><a name="t4"></a></p>
</section>
</section>
<section id="task-4-unsupervised-learning-topic-modeling">
<h2>Task 4: Unsupervised Learning - Topic Modeling<a class="headerlink" href="#task-4-unsupervised-learning-topic-modeling" title="Permalink to this headline">#</a></h2>
<p>Topic modelling is a technique used in NLP that aims to identify the underlying topics or themes in a collection of texts. One way to perform topic modelling is using the probabilistic model Latent Dirichlet Allocation (LDA).</p>
<p>LDA assumes that each document in a collection is a mixture of different topics, and each topic is a probability distribution over a set of words. The model then infers the underlying topic distribution for each document in the collection and the word distribution for each topic. LDA is trained using an iterative algorithm that maximizes the likelihood of observing the given documents.</p>
<p>To use LDA, we need to represent the documents as a bag of words, where the order of the words is ignored and only the frequency of each word in the document is considered. This bag-of-words representation allows us to represent each document as a vector of word frequencies, which can be used as input to the LDA algorithm. Computing LDA might take a moment on our dataset size.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert preprocessed text to bag-of-words representation using CountVectorizer.</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We will use the <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code> function in the vectorizer. But in this case, we need a string that represents a sentence as the input. So, we can just join all the tokens together into one string. We also reset the index for consistency.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_strings</span> <span class="o">=</span> <span class="n">df_with_nltk_tokens_and_spacy_tokens</span><span class="p">[</span><span class="s2">&quot;spacy_tokens&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">df_strings</span> <span class="o">=</span> <span class="n">df_strings</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df_strings</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0       Wall Bears Claw Black Reuters Reuters short se...
1       Carlyle look Commercial Aerospace Reuters Reut...
2       oil Economy Cloud Stocks Outlook Reuters Reute...
3       Iraq Halts Oil Exports Main Southern Pipeline ...
4       oil price soar time record pose new menace eco...
                              ...                        
3995    IOA set committee probe dope scandal Athens Au...
3996    FACTBOX Jonathan Woodgate factbox MADRID Aug R...
3997    british canoe pair lose ATHENS Reuters Slovaki...
3998    Softball Team Posts Shutout AP AP Cat Osterman...
3999    Paul Hamm example PAUL HAMM fall rise Olympics...
Name: spacy_tokens, Length: 4000, dtype: object
</pre></div>
</div>
</div>
</div>
<p>Then, we can use the <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code> function to get the bag of words vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_strings</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We convert the original matrix to a data frame to make it easier to see the bag of words. The columns indicate tokens, and the values for each cell indicate the word counts. The number of columns in the data frame matches the <code class="docutils literal notranslate"><span class="pre">max_features</span></code> parameter in the <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code>. The number of rows matches the size of the training data.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>accept</th>
      <th>accord</th>
      <th>accounting</th>
      <th>accuse</th>
      <th>action</th>
      <th>activity</th>
      <th>ad</th>
      <th>add</th>
      <th>advance</th>
      <th>afghanistan</th>
      <th>...</th>
      <th>wrap</th>
      <th>xinhuanet</th>
      <th>xp</th>
      <th>yahoo</th>
      <th>yankees</th>
      <th>year</th>
      <th>yesterday</th>
      <th>york</th>
      <th>young</th>
      <th>yukos</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>3995</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3996</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3997</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3998</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3999</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>4000 rows × 1000 columns</p>
</div></div></div>
</div>
<p>Now we have the bag of words vector. We can use the vector for LDA topic modeling.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the number of topics to model with LDA.</span>
<span class="n">num_topics</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Fit LDA to the feature matrix. Verbose so we know what iteration we are on.</span>
<span class="c1"># The random state is just for producing consistent results.</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">num_topics</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>iteration: 1 of max_iter: 10
iteration: 2 of max_iter: 10
iteration: 3 of max_iter: 10
iteration: 4 of max_iter: 10
iteration: 5 of max_iter: 10
iteration: 6 of max_iter: 10
iteration: 7 of max_iter: 10
iteration: 8 of max_iter: 10
iteration: 9 of max_iter: 10
iteration: 10 of max_iter: 10
</pre></div>
</div>
</div>
</div>
<p>Now we can check the topic vectors in the LDA model. Each vector represents the topic in a high dimensional space. The high dimensional space is formed by using the word tokens. So, the vectors can also be viewed as weights that represents the number of importance that a word token was assigned to the topic. In the following code block, we print the shape of the vectors. The row size should match the number of topics that we set before. The column size should match the <code class="docutils literal notranslate"><span class="pre">max_features</span></code> parameter, which means the number of words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 1000)
</pre></div>
</div>
</div>
</div>
<p><a name="a4"></a></p>
<section id="assignment-for-task-4">
<h3>Assignment for Task 4<a class="headerlink" href="#assignment-for-task-4" title="Permalink to this headline">#</a></h3>
<p>We want to get the weights for each word in each topic and visualize them using word clouds. In the above case, the shape should be <code class="docutils literal notranslate"><span class="pre">(4,</span> <span class="pre">1000)</span></code>, which means we have 4 topics, and each topic is represented by a distribution (i.e., weights) of 1000 words. To make the world cloud visualization simple, we only wants to use the top <code class="docutils literal notranslate"><span class="pre">n</span></code> number of words with the highest weights.</p>
<p><strong>Your task (which is your assignment) is to write functions to do the following:</strong></p>
<ul class="simple">
<li><p>Given a 1D NumPy array, return the indexes of the top <code class="docutils literal notranslate"><span class="pre">n</span></code> number of items according to their values. In other words, we want the indexes that can help us select the highest <code class="docutils literal notranslate"><span class="pre">n</span></code> values. For example, for <code class="docutils literal notranslate"><span class="pre">n=3</span></code> in array <code class="docutils literal notranslate"><span class="pre">[3,1,2,4,0]</span></code>, the function should return <code class="docutils literal notranslate"><span class="pre">[3,0,2]</span></code>, because the highest value is <code class="docutils literal notranslate"><span class="pre">4</span></code> with index <code class="docutils literal notranslate"><span class="pre">3</span></code> in the original array, and so on.</p>
<ul>
<li><p>Hint: use the <code class="docutils literal notranslate"><span class="pre">numpy.argsort</span></code> function.</p></li>
</ul>
</li>
<li><p>Notice that the <code class="docutils literal notranslate"><span class="pre">numpy.argsort</span></code> function gives you the indexes from the array items with the lowest value, which is not what we want. You need to figure out a way to reverse a numpy array and select the top <code class="docutils literal notranslate"><span class="pre">n</span></code> items.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_index_of_top_n_items</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given an NumPy array, return the indexes of the top &quot;n&quot; number of items according to their values.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    array : numpy.ndarray</span>
<span class="sd">        A 1D NumPy array.</span>
<span class="sd">    n : int</span>
<span class="sd">        The top &quot;n&quot; number of items that we want.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    numpy.ndarray</span>
<span class="sd">        The indexes of the top &quot;n&quot; items.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">###################################</span>
    <span class="c1"># Fill in your answer here</span>
    <span class="k">return</span> <span class="kc">None</span>
    <span class="c1">###################################</span>
</pre></div>
</div>
</div>
</div>
<p>The following code shows the example that we mentioned above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">answer_top_n_for_A</span> <span class="o">=</span> <span class="n">answer_get_index_of_top_n_items</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">answer_top_n_for_A</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([3, 0, 2])
</pre></div>
</div>
</div>
</div>
<p>The code below tests if the output of your function matches the expected output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">B</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">top_n_for_topic_0</span> <span class="o">=</span> <span class="n">get_index_of_top_n_items</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">answer_top_n_for_topic_0</span> <span class="o">=</span> <span class="n">answer_get_index_of_top_n_items</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">check_answer_np</span><span class="p">(</span><span class="n">top_n_for_topic_0</span><span class="p">,</span> <span class="n">answer_top_n_for_topic_0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test case 1 failed.
Your output is:
None
Expected output is:
[ 38  56 602 978 361 884 526 747 995 603]
</pre></div>
</div>
</div>
</div>
<p>We can now use the function that we just implemented in the following function to help us get the weights for the top <code class="docutils literal notranslate"><span class="pre">n</span></code> words for each topic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_word_weights_for_topics</span><span class="p">(</span><span class="n">lda_model</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get weights for words for each topic.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lda_model : sklearn.decomposition.LatentDirichletAllocation</span>
<span class="sd">        The LDA model.</span>
<span class="sd">    vectorizer : sklearn.feature_extraction.text.CountVectorizer</span>
<span class="sd">        The count vectorizer.</span>
<span class="sd">    n : int</span>
<span class="sd">        Number of important words that we want to get.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict of pandas.DataFrame</span>
<span class="sd">        A dictionary with data frames.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="k">if</span> <span class="n">n</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">n</span>
    <span class="n">topic_word_weights</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">topic_vector</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lda_model</span><span class="o">.</span><span class="n">components_</span><span class="p">):</span>
        <span class="n">top_features_ind</span> <span class="o">=</span> <span class="n">answer_get_index_of_top_n_items</span><span class="p">(</span><span class="n">topic_vector</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="n">top_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">top_features_ind</span><span class="p">]</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">topic_vector</span><span class="p">[</span><span class="n">top_features_ind</span><span class="p">]</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">top_features</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">])</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">topic_word_weights</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span>

    <span class="k">return</span> <span class="n">topic_word_weights</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can take a look at the data first. For simplicity, we only print the first 10 important words for each topic.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">topic_word_weights</span> <span class="o">=</span> <span class="n">get_word_weights_for_topics</span><span class="p">(</span><span class="n">lda</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">topic_word_weights</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Topic #</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]))</span>
    <span class="n">display</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic #0:
ap athens olympic win gold team man reuters year olympics
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>ap</th>
      <td>403.515105</td>
    </tr>
    <tr>
      <th>athens</th>
      <td>403.246789</td>
    </tr>
    <tr>
      <th>olympic</th>
      <td>309.246800</td>
    </tr>
    <tr>
      <th>win</th>
      <td>286.162784</td>
    </tr>
    <tr>
      <th>gold</th>
      <td>243.246784</td>
    </tr>
    <tr>
      <th>team</th>
      <td>210.578474</td>
    </tr>
    <tr>
      <th>man</th>
      <td>199.276155</td>
    </tr>
    <tr>
      <th>reuters</th>
      <td>190.315506</td>
    </tr>
    <tr>
      <th>year</th>
      <td>182.915320</td>
    </tr>
    <tr>
      <th>olympics</th>
      <td>175.246315</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic #1:
reuters price oil google new say stock share york high
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>reuters</th>
      <td>613.090454</td>
    </tr>
    <tr>
      <th>price</th>
      <td>447.227585</td>
    </tr>
    <tr>
      <th>oil</th>
      <td>333.991865</td>
    </tr>
    <tr>
      <th>google</th>
      <td>330.413708</td>
    </tr>
    <tr>
      <th>new</th>
      <td>327.240235</td>
    </tr>
    <tr>
      <th>say</th>
      <td>302.630197</td>
    </tr>
    <tr>
      <th>stock</th>
      <td>251.689804</td>
    </tr>
    <tr>
      <th>share</th>
      <td>240.112990</td>
    </tr>
    <tr>
      <th>york</th>
      <td>220.173493</td>
    </tr>
    <tr>
      <th>high</th>
      <td>214.368258</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic #2:
new ap company say microsoft security software service plan time
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>new</th>
      <td>294.138710</td>
    </tr>
    <tr>
      <th>ap</th>
      <td>211.640513</td>
    </tr>
    <tr>
      <th>company</th>
      <td>203.847837</td>
    </tr>
    <tr>
      <th>say</th>
      <td>197.105016</td>
    </tr>
    <tr>
      <th>microsoft</th>
      <td>128.118405</td>
    </tr>
    <tr>
      <th>security</th>
      <td>118.300459</td>
    </tr>
    <tr>
      <th>software</th>
      <td>117.660863</td>
    </tr>
    <tr>
      <th>service</th>
      <td>109.950400</td>
    </tr>
    <tr>
      <th>plan</th>
      <td>107.479263</td>
    </tr>
    <tr>
      <th>time</th>
      <td>107.182716</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic #3:
say reuters ap iraq najaf president plan kill minister wednesday
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>say</th>
      <td>257.517882</td>
    </tr>
    <tr>
      <th>reuters</th>
      <td>240.920321</td>
    </tr>
    <tr>
      <th>ap</th>
      <td>225.568027</td>
    </tr>
    <tr>
      <th>iraq</th>
      <td>168.908879</td>
    </tr>
    <tr>
      <th>najaf</th>
      <td>157.249665</td>
    </tr>
    <tr>
      <th>president</th>
      <td>154.238458</td>
    </tr>
    <tr>
      <th>plan</th>
      <td>150.074533</td>
    </tr>
    <tr>
      <th>kill</th>
      <td>133.206681</td>
    </tr>
    <tr>
      <th>minister</th>
      <td>132.223089</td>
    </tr>
    <tr>
      <th>wednesday</th>
      <td>110.130430</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Then, we can use the word weights to create word clouds.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate a word cloud for each topic.</span>
<span class="k">for</span> <span class="n">topic_idx</span><span class="p">,</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">topic_word_weights</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">frequencies</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">words</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">words</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]))</span>
    <span class="n">wordcloud</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span><span class="n">background_color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span><span class="o">.</span><span class="n">generate_from_frequencies</span><span class="p">(</span><span class="n">frequencies</span><span class="p">)</span>
    <span class="c1"># Display image</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Topic </span><span class="si">{</span><span class="n">topic_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wordcloud</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/tutorial-text-data_107_0.png" src="../_images/tutorial-text-data_107_0.png" />
<img alt="../_images/tutorial-text-data_107_1.png" src="../_images/tutorial-text-data_107_1.png" />
<img alt="../_images/tutorial-text-data_107_2.png" src="../_images/tutorial-text-data_107_2.png" />
<img alt="../_images/tutorial-text-data_107_3.png" src="../_images/tutorial-text-data_107_3.png" />
</div>
</div>
<p>Compare this with the word cloud visualizations in the pre-processing step previously. Does the LDA topic modeling represent the actural four document classes in the training data? What do you think?</p>
<p>For this task, we mainly use a qualitative way to evaluate topic modeling by visually inspecting the word clouds. There are also quantiative ways to evaluate the models, but they are not covered in this course. If you are interested in this, check the following resources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_tutorial.ipynb">Demonstration of the topic coherence pipeline in Gensim</a></p></li>
<li><p><a class="reference external" href="https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb">Performing Model Selection Using Topic Coherence</a></p></li>
<li><p><a class="reference external" href="https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/topic_coherence-movies.ipynb">Benchmark testing of coherence pipeline on Movies dataset</a></p></li>
</ul>
<p><a name="t5"></a></p>
</section>
</section>
<section id="task-5-supervised-learning-topic-classification">
<h2>Task 5: Supervised Learning - Topic Classification<a class="headerlink" href="#task-5-supervised-learning-topic-classification" title="Permalink to this headline">#</a></h2>
<p>Topic classification is a task in NLP that involves automatically assigning a given text document to one or more predefined categories or topics. This task is essential for various applications, such as document organization, search engines, sentiment analysis, and more.</p>
<p>In recent years, deep learning models have shown remarkable performance in various NLP tasks, including topic classification. We will explore a neural network-based approach for topic classification using the PyTorch framework. PyTorch provides an efficient way to build and train neural networks with a high degree of flexibility and ease of use.</p>
<p><a name="t5-1"></a></p>
<section id="compute-word-embeddings">
<h3>Compute Word Embeddings<a class="headerlink" href="#compute-word-embeddings" title="Permalink to this headline">#</a></h3>
<p>We will first look at word embeddings, which represent words as vectors in a high-dimensional space. The key idea behind word embeddings is that words with similar meanings tend to appear in similar contexts, and therefore their vector representations should be close together in this high-dimensional space. Word embeddings have been widely used in various NLP tasks such as sentiment analysis, machine translation, and information retrieval.</p>
<p>There are several techniques to generate word embeddings, but one of the most popular methods is the Word2Vec algorithm, which is based on a neural network architecture. Word2Vec learns embeddings by predicting the probability of a word given its context (continuous bag of words or skip-gram model). The output of the network is a set of word vectors that can be used as embeddings.</p>
<p>We can train a Word2Vec model ourselves, but keep in mind that later on, it’s not nice if we don’t have embeddings for certain words in the test set. So let’s first apply the familiar preprocessing steps to the test set:</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reformat the test set.</span>
<span class="n">df_test_reformat</span> <span class="o">=</span> <span class="n">reformat_data</span><span class="p">(</span><span class="n">df_test</span><span class="p">)</span>

<span class="c1"># NLTK preprocessing.</span>
<span class="n">df_test_with_tokens</span> <span class="o">=</span> <span class="n">answer_tokenize_and_lemmatize</span><span class="p">(</span><span class="n">df_test_reformat</span><span class="p">)</span>

<span class="c1"># spaCy preprocessing.</span>
<span class="n">df_test_with_nltk_tokens_and_spacy_tokens</span> <span class="o">=</span> <span class="n">add_spacy_tokens</span><span class="p">(</span><span class="n">add_spacy_doc</span><span class="p">(</span><span class="n">df_test_with_tokens</span><span class="p">,</span> <span class="n">nlp</span><span class="p">))</span>

<span class="n">display</span><span class="p">(</span><span class="n">df_test_with_nltk_tokens_and_spacy_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "7c0ce75d9072407eb37fb8ac366051a6", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "93c084a874054aac9d37dfe431c1c9d4", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "5897f0dc6aea48b78b6dcc41757a3f17", "version_major": 2, "version_minor": 0}
</script><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class_idx</th>
      <th>class</th>
      <th>text</th>
      <th>tokens</th>
      <th>doc</th>
      <th>spacy_tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>Business</td>
      <td>Fears for T N pension after talks Unions repre...</td>
      <td>[Fears, for, T, N, pension, after, talk, Union...</td>
      <td>(Fears, for, T, N, pension, after, talks, Unio...</td>
      <td>[fear, t, N, pension, talk, Unions, represent,...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>The Race is On: Second Private Team Sets Launc...</td>
      <td>[The, Race, be, On, :, Second, Private, Team, ...</td>
      <td>(The, Race, is, On, :, Second, Private, Team, ...</td>
      <td>[race, Second, private, Team, Sets, Launch, Da...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>Ky. Company Wins Grant to Study Peptides (AP) ...</td>
      <td>[Ky., Company, Wins, Grant, to, Study, Peptide...</td>
      <td>(Ky., Company, Wins, Grant, to, Study, Peptide...</td>
      <td>[Company, win, Grant, Study, Peptides, AP, AP,...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>Prediction Unit Helps Forecast Wildfires (AP) ...</td>
      <td>[Prediction, Unit, Helps, Forecast, Wildfires,...</td>
      <td>(Prediction, Unit, Helps, Forecast, Wildfires,...</td>
      <td>[prediction, Unit, help, Forecast, Wildfires, ...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>Calif. Aims to Limit Farm-Related Smog (AP) AP...</td>
      <td>[Calif, ., Aims, to, Limit, Farm-Related, Smog...</td>
      <td>(Calif., Aims, to, Limit, Farm, -, Related, Sm...</td>
      <td>[aim, Limit, Farm, relate, Smog, AP, AP, South...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>473</th>
      <td>3</td>
      <td>Business</td>
      <td>New Overtime Rules Take Effect New Bush admini...</td>
      <td>[New, Overtime, Rules, Take, Effect, New, Bush...</td>
      <td>(New, Overtime, Rules, Take, Effect, New, Bush...</td>
      <td>[New, Overtime, Rules, Effect, New, Bush, admi...</td>
    </tr>
    <tr>
      <th>479</th>
      <td>3</td>
      <td>Business</td>
      <td>Dollar Holds Gains, Fed Comments Help  TOKYO (...</td>
      <td>[Dollar, Holds, Gains, ,, Fed, Comments, Help,...</td>
      <td>(Dollar, Holds, Gains, ,, Fed, Comments, Help,...</td>
      <td>[Dollar, Holds, Gains, Fed, comment, help, TOK...</td>
    </tr>
    <tr>
      <th>481</th>
      <td>3</td>
      <td>Business</td>
      <td>Dark arts of spin evident in phoney war for Ab...</td>
      <td>[Dark, art, of, spin, evident, in, phoney, war...</td>
      <td>(Dark, arts, of, spin, evident, in, phoney, wa...</td>
      <td>[dark, art, spin, evident, phoney, war, Abbey,...</td>
    </tr>
    <tr>
      <th>482</th>
      <td>3</td>
      <td>Business</td>
      <td>Controversial US Overtime Rules Take Effect Ne...</td>
      <td>[Controversial, US, Overtime, Rules, Take, Eff...</td>
      <td>(Controversial, US, Overtime, Rules, Take, Eff...</td>
      <td>[controversial, Overtime, Rules, Effect, new, ...</td>
    </tr>
    <tr>
      <th>484</th>
      <td>3</td>
      <td>Business</td>
      <td>SAS Braathens to cut Gatwick, Geneva flights b...</td>
      <td>[SAS, Braathens, to, cut, Gatwick, ,, Geneva, ...</td>
      <td>(SAS, Braathens, to, cut, Gatwick, ,, Geneva, ...</td>
      <td>[SAS, Braathens, cut, Gatwick, Geneva, flight,...</td>
    </tr>
  </tbody>
</table>
<p>400 rows × 6 columns</p>
</div></div></div>
</div>
<p>To obtain the complete model, we combine the <code class="docutils literal notranslate"><span class="pre">tokens</span></code> column into one series and call the <code class="docutils literal notranslate"><span class="pre">Word2Vec</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Rename the very long variables</span>
<span class="n">df_train_preprocessd</span> <span class="o">=</span> <span class="n">df_with_nltk_tokens_and_spacy_tokens</span>
<span class="n">df_test_preprocessd</span> <span class="o">=</span> <span class="n">df_test_with_nltk_tokens_and_spacy_tokens</span>

<span class="c1"># Get all tokens into one series.</span>
<span class="n">tokens_both</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_train_preprocessd</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">],</span> <span class="n">df_test_preprocessd</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]])</span>

<span class="c1"># Train a Word2Vec model on the NLTK tokens.</span>
<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">tokens_both</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To obtain the embeddings, we can use the <code class="docutils literal notranslate"><span class="pre">Word2Vec.wv[word]</span></code> syntax. To get multiple vectors nicely next to each other in a 2D matrix, we can call <code class="docutils literal notranslate"><span class="pre">numpy.vstack</span></code>.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;rain&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">]]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[-2.60392636e-01  3.61957699e-02  1.09209269e-01  1.59883693e-01
   7.47058615e-02 -1.83978200e-01  1.15174890e-01  1.41995341e-01
  -1.27571225e-01  4.41441536e-01 -7.66160041e-02 -4.21129227e-01
  -7.70293251e-02 -1.32060111e-01  4.39062655e-01  2.57349819e-01
  -6.41567782e-02  1.71880454e-01  6.53994977e-02 -2.28234261e-01
  -1.31064489e-01  1.52635470e-03  2.27316573e-01  1.49013713e-01
  -2.45736703e-01  1.41700311e-02 -9.18157026e-02  3.44747216e-01
  -3.31136227e-01 -1.63988218e-01  3.18268031e-01 -1.14187524e-01
  -9.01155993e-02 -3.15312743e-01 -2.72045583e-02  1.96554258e-01
   7.19608963e-01 -4.33225513e-01 -3.80716771e-01 -3.01050067e-01]
 [-5.55762206e-04  1.75153557e-02  1.63081214e-02  4.37505618e-02
   1.91775504e-02 -2.68786456e-02 -9.65814292e-03  4.77873608e-02
   1.68873817e-02  7.72785023e-02 -1.24694523e-03 -4.48038988e-02
  -3.69028486e-02  1.08696317e-04  5.88426664e-02  4.69341688e-02
  -9.38272290e-03  8.82259570e-03  2.31929235e-02 -4.38504890e-02
  -1.66977551e-02 -2.26898771e-02  3.22737843e-02  2.31878553e-02
  -1.98843461e-02  5.50923310e-03  1.77841890e-03  4.55366671e-02
  -2.32978743e-02 -3.88195775e-02  6.80615827e-02 -2.95684878e-02
  -2.69708876e-02 -5.04455604e-02 -1.28503460e-02  2.98611298e-02
   1.10825963e-01 -7.42685422e-02 -6.67480677e-02 -5.01767062e-02]
 [-7.88029432e-02  3.57245207e-02  1.87077932e-02  8.16702172e-02
  -3.05961762e-02 -5.07439226e-02  3.81175242e-02  3.65127474e-02
  -5.89498617e-02  1.90558270e-01 -2.58215386e-02 -1.45636722e-01
  -3.71700376e-02 -7.68104494e-02  1.64298996e-01  1.00462683e-01
   7.34513486e-03  2.07834784e-02  4.85605095e-03 -8.80082399e-02
  -4.16397192e-02 -2.22325884e-02  1.00829713e-01  1.42552294e-02
  -7.83072636e-02 -1.89409941e-03 -3.98297459e-02  1.38811961e-01
  -1.12817809e-01 -3.60937640e-02  1.17614634e-01 -6.38329387e-02
  -2.99992543e-02 -8.16034526e-02 -3.84166278e-02  1.12850294e-01
   3.21655065e-01 -1.53566912e-01 -1.89866170e-01 -6.67793155e-02]]
</pre></div>
</div>
</div>
</div>
<p>The spaCy model we used has a <code class="docutils literal notranslate"><span class="pre">Tok2Vec</span></code> algorithm in its pipeline, so we can directly access the 2D matrix of all word vectors on a document with the <code class="docutils literal notranslate"><span class="pre">Doc.tensor</span></code> attribute. Keep in mind this still contains the embeddings of the stopwords.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 0.44236845 -0.01180173  0.20908105 ... -0.18478878 -0.17872915
  -0.20212969]
 [ 1.347008   -0.9065002   1.0384194  ...  0.9311755  -0.88710904
  -1.0599288 ]
 [ 0.84580296 -1.3604947   0.8471415  ...  0.95768267 -0.07491717
   0.32537135]
 ...
 [ 1.4835097   0.13096532  0.63991284 ... -1.166823    0.250414
  -0.7703706 ]
 [-0.18388712 -0.7047122   0.49538338 ...  0.26794562 -0.6400709
   0.57972735]
 [-0.44166976 -0.5572643  -0.5958953  ... -0.62469536 -0.9879791
  -0.19887315]]
</pre></div>
</div>
</div>
</div>
<p>To prepare the word embeddings for classification, we will add a <code class="docutils literal notranslate"><span class="pre">tensor</span></code> column to both the dataframes for training and testing. Each cell in the <code class="docutils literal notranslate"><span class="pre">tensor</span></code> column should be a tensor array, representing the word embedding vector for the text in the corresponding row. The tensors need to have the same size for both the training and test sets, so we also need to pad the tensors with smaller sizes by adding zeros at the end. We provide a <code class="docutils literal notranslate"><span class="pre">add_padded_tensors</span></code> function in the <a class="reference external" href="#util">utility file</a> for doing this.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_train_with_tensor</span><span class="p">,</span> <span class="n">df_test_with_tensor</span> <span class="o">=</span> <span class="n">add_padded_tensors</span><span class="p">(</span><span class="n">df_train_preprocessd</span><span class="p">,</span> <span class="n">df_test_preprocessd</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df_test_with_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class_idx</th>
      <th>class</th>
      <th>text</th>
      <th>tokens</th>
      <th>doc</th>
      <th>spacy_tokens</th>
      <th>tensor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>Business</td>
      <td>Fears for T N pension after talks Unions repre...</td>
      <td>[Fears, for, T, N, pension, after, talk, Union...</td>
      <td>(Fears, for, T, N, pension, after, talks, Unio...</td>
      <td>[fear, t, N, pension, talk, Unions, represent,...</td>
      <td>[[-0.9206627, 0.7367932, -0.17732513, 1.061838...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>The Race is On: Second Private Team Sets Launc...</td>
      <td>[The, Race, be, On, :, Second, Private, Team, ...</td>
      <td>(The, Race, is, On, :, Second, Private, Team, ...</td>
      <td>[race, Second, private, Team, Sets, Launch, Da...</td>
      <td>[[0.3289305, -0.659908, -0.04628387, 0.459979,...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>Ky. Company Wins Grant to Study Peptides (AP) ...</td>
      <td>[Ky., Company, Wins, Grant, to, Study, Peptide...</td>
      <td>(Ky., Company, Wins, Grant, to, Study, Peptide...</td>
      <td>[Company, win, Grant, Study, Peptides, AP, AP,...</td>
      <td>[[-0.085725844, -1.1698873, 0.5357769, 1.32863...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>Prediction Unit Helps Forecast Wildfires (AP) ...</td>
      <td>[Prediction, Unit, Helps, Forecast, Wildfires,...</td>
      <td>(Prediction, Unit, Helps, Forecast, Wildfires,...</td>
      <td>[prediction, Unit, help, Forecast, Wildfires, ...</td>
      <td>[[-0.092530996, -1.1091797, 0.007843953, 0.410...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>Sci/Tech</td>
      <td>Calif. Aims to Limit Farm-Related Smog (AP) AP...</td>
      <td>[Calif, ., Aims, to, Limit, Farm-Related, Smog...</td>
      <td>(Calif., Aims, to, Limit, Farm, -, Related, Sm...</td>
      <td>[aim, Limit, Farm, relate, Smog, AP, AP, South...</td>
      <td>[[-0.10481978, -0.5771505, 1.3032898, 0.098989...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>473</th>
      <td>3</td>
      <td>Business</td>
      <td>New Overtime Rules Take Effect New Bush admini...</td>
      <td>[New, Overtime, Rules, Take, Effect, New, Bush...</td>
      <td>(New, Overtime, Rules, Take, Effect, New, Bush...</td>
      <td>[New, Overtime, Rules, Effect, New, Bush, admi...</td>
      <td>[[0.63283896, -0.8598237, 0.18151897, 0.718614...</td>
    </tr>
    <tr>
      <th>479</th>
      <td>3</td>
      <td>Business</td>
      <td>Dollar Holds Gains, Fed Comments Help  TOKYO (...</td>
      <td>[Dollar, Holds, Gains, ,, Fed, Comments, Help,...</td>
      <td>(Dollar, Holds, Gains, ,, Fed, Comments, Help,...</td>
      <td>[Dollar, Holds, Gains, Fed, comment, help, TOK...</td>
      <td>[[0.27932516, -1.1461885, 0.29283038, 0.256060...</td>
    </tr>
    <tr>
      <th>481</th>
      <td>3</td>
      <td>Business</td>
      <td>Dark arts of spin evident in phoney war for Ab...</td>
      <td>[Dark, art, of, spin, evident, in, phoney, war...</td>
      <td>(Dark, arts, of, spin, evident, in, phoney, wa...</td>
      <td>[dark, art, spin, evident, phoney, war, Abbey,...</td>
      <td>[[0.6209623, -0.8559445, -0.29310828, 0.090776...</td>
    </tr>
    <tr>
      <th>482</th>
      <td>3</td>
      <td>Business</td>
      <td>Controversial US Overtime Rules Take Effect Ne...</td>
      <td>[Controversial, US, Overtime, Rules, Take, Eff...</td>
      <td>(Controversial, US, Overtime, Rules, Take, Eff...</td>
      <td>[controversial, Overtime, Rules, Effect, new, ...</td>
      <td>[[0.19809127, -0.7492492, 0.23203318, 0.294419...</td>
    </tr>
    <tr>
      <th>484</th>
      <td>3</td>
      <td>Business</td>
      <td>SAS Braathens to cut Gatwick, Geneva flights b...</td>
      <td>[SAS, Braathens, to, cut, Gatwick, ,, Geneva, ...</td>
      <td>(SAS, Braathens, to, cut, Gatwick, ,, Geneva, ...</td>
      <td>[SAS, Braathens, cut, Gatwick, Geneva, flight,...</td>
      <td>[[-0.34229928, -0.94983363, 0.3551279, 0.64431...</td>
    </tr>
  </tbody>
</table>
<p>400 rows × 7 columns</p>
</div></div></div>
</div>
<p><a name="t5-2"></a></p>
</section>
<section id="build-the-classifier">
<h3>Build the Classifier<a class="headerlink" href="#build-the-classifier" title="Permalink to this headline">#</a></h3>
<p>Our neural network will take the embedding representation of the document as input and predict the corresponding topic using a softmax output layer. We will evaluate the performance of our model using various metrics such as accuracy, precision, recall, and F1-score.</p>
<p>The following code demonstrates how to implement a neural network for topic classification in PyTorch. First let’s do some more preparations for our inputs, turning them into PyTorch tensors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Transform spaCy tensors into PyTorch tensors.</span>
<span class="n">input_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">df_train_with_tensor</span><span class="p">[</span><span class="s2">&quot;tensor&quot;</span><span class="p">]))</span>
<span class="n">input_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">df_test_with_tensor</span><span class="p">[</span><span class="s2">&quot;tensor&quot;</span><span class="p">]))</span>

<span class="c1"># Get the labels, move to 0-indexed instead of 1-indexed.</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">df_train_with_tensor</span><span class="p">[</span><span class="s2">&quot;class_idx&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">df_test_with_tensor</span><span class="p">[</span><span class="s2">&quot;class_idx&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># One-hot encode labels for training.</span>
<span class="n">train_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">train_labels</span><span class="p">),</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">train_target</span> <span class="o">=</span> <span class="n">train_target</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">train_labels</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Then, it is time to define our network. The neural net consists of three fully connected layers (<code class="docutils literal notranslate"><span class="pre">fc1</span></code>, <code class="docutils literal notranslate"><span class="pre">fc2</span></code>, and <code class="docutils literal notranslate"><span class="pre">fc3</span></code>) with ReLU activation (<code class="docutils literal notranslate"><span class="pre">relu</span></code>) in between each layer. We flatten the input tensor using <code class="docutils literal notranslate"><span class="pre">view</span></code> before passing it through the fully connected layers. Finally, we apply the softmax activation function (<code class="docutils literal notranslate"><span class="pre">softmax</span></code>) to the output tensor to obtain the predicted probabilities for each class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TopicClassifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_width</span><span class="p">,</span> <span class="n">input_length</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TopicClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_width</span> <span class="o">=</span> <span class="n">input_width</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_length</span> <span class="o">=</span> <span class="n">input_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_width</span> <span class="o">*</span> <span class="n">input_length</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Flatten the input tensor.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_width</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_length</span><span class="p">)</span>

        <span class="c1"># Pass through the fully connected layers with ReLU activation.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Apply softmax activation to the output.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Now it’s time to train our network, this may take a while, but the current loss will be printed after every epoch.
If you want to run the code faster, you can also put this notebook on Google Colab and use its provided GPU to speed up computing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define parameters.</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_labels</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="n">input_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="c1"># Define model, loss function and optimizer.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TopicClassifier</span><span class="p">(</span><span class="o">*</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="n">n_classes</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># Training loop.</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">input_train</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch [</span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s2">], Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch [1/5], Loss: 1.3852
Epoch [2/5], Loss: 1.3765
Epoch [3/5], Loss: 1.3249
Epoch [4/5], Loss: 1.0777
Epoch [5/5], Loss: 0.8335
</pre></div>
</div>
</div>
</div>
<p><a name="a5"></a></p>
</section>
<section id="optional-assignment-for-task-5">
<h3>Optional Assignment for Task 5<a class="headerlink" href="#optional-assignment-for-task-5" title="Permalink to this headline">#</a></h3>
<p>The following code evaluates the model using a confusion matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate the neural net on the test set.</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Sample from the model.</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">test_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>
    <span class="c1"># Reuse our previous function to get the label with biggest probability.</span>
    <span class="n">test_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test_outputs</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Set model back to training mode.</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># Compute the confusion matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">test_pred</span><span class="p">)</span>

<span class="c1"># Plot the confusion matrix using seaborn</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;World&quot;</span><span class="p">,</span> <span class="s2">&quot;Sports&quot;</span><span class="p">,</span> <span class="s2">&quot;Business&quot;</span><span class="p">,</span> <span class="s2">&quot;Sci/Tech&quot;</span><span class="p">]</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Blues&quot;</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted Labels&quot;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;True Labels&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/tutorial-text-data_135_0.png" src="../_images/tutorial-text-data_135_0.png" />
</div>
</div>
<p>If you do not feel done with text data yet, there is always more to do. In this optional assignment, you can experiment with the number of epochs, learning rate, vector size, optimizer, neural network architecture, regularization, etc. Also, we only use a small subset of this dataset for performance issues. If you have a high-end computer, you can go to the beginning of this tutorial to increase the size of the subset.</p>
<p>Even during the preprocessing, we could have done some things differently, like making everything lowercase and removing punctuation. Be aware that every choice you make along the way trickles down into your pipeline and can have some effect on your results. Also, take the time to write the code to evaluate the model with more metrics, such as accuracy, precision, recall, and the F1 score.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="credit"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Credit: this teaching material is created by <a class="reference external" href="https://github.com/robertvanstraten">Robert van Straten</a> and revised by <a class="reference external" href="https://github.com/amonroym99">Alejandro Monroy</a> under the supervision of <a class="reference external" href="https://github.com/yenchiah">Yen-Chia Hsu</a>.</p>
</dd>
</dl>
</section>
</section>
</section>


<script type="application/vnd.jupyter.widget-state+json">
{"state": {"0c7b72d364c44540a11c5367ebce9923": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "0e59296785514778850fdb3aab855d68": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "103a910c98584ecbada337b2abe5c594": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"layout": "IPY_MODEL_3c8b2e312ecd4955a2c3ab4fd8ae90dd", "style": "IPY_MODEL_6a9ed1caf1a24a30802a87a75cb7f778", "value": "\u20074000/4000\u2007[00:03&lt;00:00,\u20071186.35it/s]"}}, "141c807c083949a787e128addac83989": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"description_width": ""}}, "157e6363610947359e1e6ccca2712484": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"layout": "IPY_MODEL_8c4664ec974242578242167359e8181e", "style": "IPY_MODEL_9b37ddae28c6442c8c0b09faefadd98c", "value": "100%"}}, "15a088c050e441febe52e15da46299d4": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"description_width": ""}}, "18d5a1916daf4b4c9783c45a69fff4fb": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "1a5e511692374aee827d8627e4f3dbbf": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"description_width": ""}}, "1d33ba86162640228caf6a58d7cc60a8": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"description_width": ""}}, "1f42f03e02bd42e9826a7c1b965e718f": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"bar_style": "success", "layout": "IPY_MODEL_18d5a1916daf4b4c9783c45a69fff4fb", "max": 400, "style": "IPY_MODEL_2a03099d0c5449e5a735a073b9a1d133", "value": 400}}, "2608fb2dd6ff4de392a56093956bd243": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"bar_style": "success", "layout": "IPY_MODEL_f164dddd2c1d4600a798105ea207df78", "max": 4000, "style": "IPY_MODEL_337c6dbaaad644dda3d9c77a5625d2a5", "value": 4000}}, "2a03099d0c5449e5a735a073b9a1d133": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"description_width": ""}}, "2c9ebaa32e6a41cab7bc7db0482bb874": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"description_width": ""}}, "337c6dbaaad644dda3d9c77a5625d2a5": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"description_width": ""}}, "363a1a3b45ff4d2cade6a379b02325a0": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"layout": "IPY_MODEL_5f2fa5291b224dafbe1758bbd1951601", "style": "IPY_MODEL_b0f50cc817c64862b8ade40e6a7f4ae0", "value": "100%"}}, "39d20f81f4e544c79cca4b262eba7e75": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"layout": "IPY_MODEL_ed389fad84ed492f9da2efa34785cdbb", "style": "IPY_MODEL_b7cee821814a4dc0bdbe79676a6a9c33", "value": "\u20074000/4000\u2007[00:10&lt;00:00,\u20071930.87it/s]"}}, "3a5ab214cd9044ef8f68a89bd3da026d": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "3ac9958bde9e4872a0237ba6126fcc40": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"layout": "IPY_MODEL_86767bd5218d4c2e9fcaaf1a2df5527f", "style": "IPY_MODEL_141c807c083949a787e128addac83989", "value": "\u2007400/400\u2007[00:07&lt;00:00,\u200750.87it/s]"}}, "3af85f08c9804f7daa60baeaeb773003": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"description_width": ""}}, "3c8b2e312ecd4955a2c3ab4fd8ae90dd": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "3cd1298fafd3439890fbed293efc9469": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"layout": "IPY_MODEL_3a5ab214cd9044ef8f68a89bd3da026d", "style": "IPY_MODEL_3af85f08c9804f7daa60baeaeb773003", "value": "100%"}}, "432175215eee4460af3f6cff58003f23": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"layout": "IPY_MODEL_56dff3aff8384d5bb63a83f60e3cad20", "style": "IPY_MODEL_d94ade76261442e7bba403ac90001d39", "value": "\u2007400/400\u2007[00:00&lt;00:00,\u20076711.61it/s]"}}, "51ef1d2a3c6f47cfb353825febb5ec61": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "56dff3aff8384d5bb63a83f60e3cad20": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "5897f0dc6aea48b78b6dcc41757a3f17": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"children": ["IPY_MODEL_d142439dbde142c98f24233f7f4a76d6", "IPY_MODEL_6c4ff968839d465d801ec7e314be06fe", "IPY_MODEL_3ac9958bde9e4872a0237ba6126fcc40"], "layout": "IPY_MODEL_9a5fe12404574157948867a0a69553ad"}}, "58e4815d63ba49489174f1b2608c0cd4": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"layout": "IPY_MODEL_0e59296785514778850fdb3aab855d68", "style": "IPY_MODEL_dbbc4c4fb45b423dbc9f50b95b825005", "value": "100%"}}, "59a14778dc1447498ad6f3aed45b1c77": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"description_width": ""}}, "5dfca482b7e141e69d5f9c9dbbf2e511": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "5f2fa5291b224dafbe1758bbd1951601": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "60d059cd08a4463a91d825fa95960090": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"layout": "IPY_MODEL_c757948be999435cbd342363f3509b3a", "style": "IPY_MODEL_f2b8915670c04cb8a478faf364486581", "value": "\u20074000/4000\u2007[00:00&lt;00:00,\u20078787.82it/s]"}}, "64a5d31398024a208a6633ef30dd9a32": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"bar_style": "success", "layout": "IPY_MODEL_cb5067c9aca74cdfa617231a6a21009b", "max": 4000, "style": "IPY_MODEL_2c9ebaa32e6a41cab7bc7db0482bb874", "value": 4000}}, "674073f247e8482580b7b75479f70c3d": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "6a07866f45b04c3999d8b63cea2c587e": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "6a9ed1caf1a24a30802a87a75cb7f778": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"description_width": ""}}, "6c4ff968839d465d801ec7e314be06fe": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"bar_style": "success", "layout": "IPY_MODEL_6ced1bf4352840c29d8c614798e09b5a", "max": 400, "style": "IPY_MODEL_1a5e511692374aee827d8627e4f3dbbf", "value": 400}}, "6ced1bf4352840c29d8c614798e09b5a": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "7a3dd94774524cb79073959f31b47a37": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "7c0ce75d9072407eb37fb8ac366051a6": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"children": ["IPY_MODEL_58e4815d63ba49489174f1b2608c0cd4", "IPY_MODEL_1f42f03e02bd42e9826a7c1b965e718f", "IPY_MODEL_432175215eee4460af3f6cff58003f23"], "layout": "IPY_MODEL_6a07866f45b04c3999d8b63cea2c587e"}}, "8195b661d3b3494ca07a7ba40b53ab0d": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "86767bd5218d4c2e9fcaaf1a2df5527f": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "8c4664ec974242578242167359e8181e": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "93c084a874054aac9d37dfe431c1c9d4": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"children": ["IPY_MODEL_363a1a3b45ff4d2cade6a379b02325a0", "IPY_MODEL_a9ace5b6bd22456b9a64816f69817361", "IPY_MODEL_a056978e9efa4fff9f117cce58efdc27"], "layout": "IPY_MODEL_7a3dd94774524cb79073959f31b47a37"}}, "94f0f5ec75a84407b4df60bcde2fd6fe": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "97b56ab6417b49d7b677c3bfa135fe33": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"bar_style": "success", "layout": "IPY_MODEL_d190386b7b60415ebfd538917402453f", "max": 4000, "style": "IPY_MODEL_b54c286fe37a4705a056058a10ad7a15", "value": 4000}}, "9a5fe12404574157948867a0a69553ad": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "9b37ddae28c6442c8c0b09faefadd98c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"description_width": ""}}, "a056978e9efa4fff9f117cce58efdc27": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"layout": "IPY_MODEL_5dfca482b7e141e69d5f9c9dbbf2e511", "style": "IPY_MODEL_15a088c050e441febe52e15da46299d4", "value": "\u2007400/400\u2007[00:00&lt;00:00,\u20071099.50it/s]"}}, "a9ace5b6bd22456b9a64816f69817361": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"bar_style": "success", "layout": "IPY_MODEL_0c7b72d364c44540a11c5367ebce9923", "max": 400, "style": "IPY_MODEL_59a14778dc1447498ad6f3aed45b1c77", "value": 400}}, "adc2f2c5e39741b8924fcfd101159c86": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"children": ["IPY_MODEL_b7899a179676475785375b6a55c49732", "IPY_MODEL_64a5d31398024a208a6633ef30dd9a32", "IPY_MODEL_60d059cd08a4463a91d825fa95960090"], "layout": "IPY_MODEL_e3527ed92aa94207b5b2c91dfba5b039"}}, "b0f50cc817c64862b8ade40e6a7f4ae0": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"description_width": ""}}, "b54c286fe37a4705a056058a10ad7a15": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"description_width": ""}}, "b7899a179676475785375b6a55c49732": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"layout": "IPY_MODEL_8195b661d3b3494ca07a7ba40b53ab0d", "style": "IPY_MODEL_1d33ba86162640228caf6a58d7cc60a8", "value": "100%"}}, "b7cee821814a4dc0bdbe79676a6a9c33": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"description_width": ""}}, "c03ada6ea4e24483805124f8a00b7df1": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"children": ["IPY_MODEL_3cd1298fafd3439890fbed293efc9469", "IPY_MODEL_2608fb2dd6ff4de392a56093956bd243", "IPY_MODEL_39d20f81f4e544c79cca4b262eba7e75"], "layout": "IPY_MODEL_94f0f5ec75a84407b4df60bcde2fd6fe"}}, "c757948be999435cbd342363f3509b3a": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "cb5067c9aca74cdfa617231a6a21009b": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "d142439dbde142c98f24233f7f4a76d6": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"layout": "IPY_MODEL_51ef1d2a3c6f47cfb353825febb5ec61", "style": "IPY_MODEL_fbda4bacd9bc4988beb9356a00140a5b", "value": "100%"}}, "d190386b7b60415ebfd538917402453f": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "d94ade76261442e7bba403ac90001d39": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"description_width": ""}}, "dbbc4c4fb45b423dbc9f50b95b825005": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"description_width": ""}}, "e3527ed92aa94207b5b2c91dfba5b039": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "ed389fad84ed492f9da2efa34785cdbb": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "ed5c609a458e4c119b914b5145a2de28": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"children": ["IPY_MODEL_157e6363610947359e1e6ccca2712484", "IPY_MODEL_97b56ab6417b49d7b677c3bfa135fe33", "IPY_MODEL_103a910c98584ecbada337b2abe5c594"], "layout": "IPY_MODEL_674073f247e8482580b7b75479f70c3d"}}, "f164dddd2c1d4600a798105ea207df78": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {}}, "f2b8915670c04cb8a478faf364486581": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"description_width": ""}}, "fbda4bacd9bc4988beb9356a00140a5b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"description_width": ""}}}, "version_major": 2, "version_minor": 0}
</script>


    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="preparation-text-data.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Preparation (Text Data Processing)</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="assignment-text-data.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Assignment (Text Data Processing)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Yen-Chia Hsu<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>