{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial (Text Data Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Last updated: Mar 3, 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will familiarize you with the data science pipeline of processing text data. We will go through the various steps involved in the Natural Language Processing (NLP) pipeline for topic modelling and topic classification, including tokenization, lemmatization, and obtaining word embeddings. We will also build a neural network using PyTorch for multi-class topic classification using the dataset.\n",
    "The AG's News Topic Classification Dataset contains news articles from four different categories, making it a nice source of text data for NLP tasks. We will guide you through the process of understanding the dataset, implementing various NLP techniques, and building a model for classification. Below is the pipeline of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pipeline pic]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following links to jump to the tasks and assignments:\n",
    "\n",
    "[table of contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [AG's News Topic Classification Dataset](https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv) is a collection of over 1 million news articles from more than 2000 news sources. The dataset was created by selecting the 4 largest classes from the original corpus, resulting in 120,000 training samples and 7,600 testing samples. The dataset is provided by the academic community for research purposes in data mining, information retrieval, and other non-commercial activities. We will use it to demonstrate various NLP techniques on real data, and in the end make 2 models with this data. The files train.csv and test.csv contain all the training and testing samples as comma-separated values with 3 columns: class index, title, and description. Download train.csv and test.csv for the following tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put all the packages that are needed for this tutorial below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "from xml.sax import saxutils as su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below contains answers for the assignments in this tutorial. **Do not check the answers in the next cell before practicing the tasks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_df(df_result, df_answer, n=1):\n",
    "    \"\"\"\n",
    "    This function checks if two output dataframes are the same.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_result : pandas.DataFrame\n",
    "        The result from the output of a function.\n",
    "    df_answer: pandas.DataFrame\n",
    "        The expected output of the function.\n",
    "    n : int\n",
    "        The numbering of the test case.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df_answer.isinstance(list):\n",
    "            assert any([answer.equals(df_result) for answer in df_answer])\n",
    "        else:\n",
    "            assert df_answer.equals(df_result)\n",
    "        print(f\"Test case {n} passed.\")\n",
    "    except:\n",
    "        print(f\"Test case {n} failed.\")\n",
    "        print(\"\")\n",
    "        print(\"Your output is:\")\n",
    "        print(df_result)\n",
    "        print(\"\")\n",
    "        print(\"Expected output is\", end=\"\")\n",
    "        if df_answer.isinstance(list):\n",
    "            print(\" one of\", end=\"\")\n",
    "        print(\":\")\n",
    "        print(df_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Preprocess Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will preprocess the text data from the AG News Dataset. First, we need to load the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "display(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all the classes are distributed evenly in the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_df['Class Index'].value_counts(), test_df['Class Index'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the data more understandable, we will make the classes more understandable by transforming the `Class Index` column to a `class` column, containing the category of the news article. To process both the title and news text together, we will combine the `Title` and `Description` columns into one `text` column. We will just deal with the train data until the point where we need the test data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_data(df):\n",
    "    \"\"\"\n",
    "    Reformat the Class Index column to a Class column and combine\n",
    "    the Title and Description columns into a Text column.\n",
    "    Select only the new columns afterwards.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The original dataframe.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The reformatted dataframe.\n",
    "    \"\"\"\n",
    "    # Make the class column using a dictionary.\n",
    "    classes = {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}\n",
    "    df['class'] = df['Class Index'].apply(classes.get)\n",
    "    \n",
    "    # Use string concatonation for the Text column and unesacpe html characters.\n",
    "    df['text'] = (df['Title'] + ' ' + df['Description']).apply(su.unescape)\n",
    "    \n",
    "    # Select only the Class and Text columns.\n",
    "    df = df[['class', 'text']]\n",
    "    return df\n",
    "\n",
    "train_df = reformat_data(train_df)\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down a text into individual tokens, which are usually words but can also be phrases or sentences. It helps language models to understand and analyze text data by breaking it down into smaller, more manageable pieces. While it may seem like a trivial task, tokenization can be applied in multiple ways and thus be a complex and challenging task influencing NLP applications.\n",
    "\n",
    "For example, in languages like English, it is generally straightforward to identify words by using spaces as delimiters. However, there are exceptions, such as contractions like \"can't\" and hyphenated words like \"self-driving\". And in Dutch, where multiple nouns can be combined into one bigger noun without any delimiter this can be hard. How would you tokenize \"hippopotomonstrosesquippedaliofobie\"? In other languages, such as Chinese and Japanese, there are no spaces between words, so identifying word boundaries is much more difficult. \n",
    "\n",
    "To illustrate the use of tokenization, let's consider the following example, which tokenizes a sample text using the `word_tokenize` function from the NLTK package. That function uses a pre-trained tokenization model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sample text.\n",
    "text = \"The quick brown fox jumped over the lazy dog. The cats couldn't wait to sleep all day.\"\n",
    "\n",
    "# Tokenize the text.\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the text and the tokens.\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokenized text:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming / lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are two common techniques used in NLP to preprocess and normalize text data. Both techniques involve transforming words into their root form, but they differ in their approach and the level of normalization they provide.\n",
    "\n",
    "Stemming is a technique that involves reducing words to their base or stem form by removing any affixes or suffixes. For example, the stem of the word \"lazily\" would be \"lazi\". Stemming is a simple and fast technique that can be useful. However, it can also produce inaccurate or incorrect results since it does not consider the context or part of speech of the word.\n",
    "\n",
    "Lemmatization, on the other hand, is a more sophisticated technique that involves identifying the base or dictionary form of a word, also known as the lemma. Unlike stemming, lemmatization can consider the context and part of speech of the word, which can make it more accurate and reliable. In this example, the context and part of speech is not used. With lemmatization, the lemma of the word \"lazily\" would be \"lazy\". Lemmatization can be slower and more complex than stemming but provides a higher level of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the stemmer and lemmatizer.\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Perform stemming and lemmatization seperately on the tokens.\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Print the results.\n",
    "print(\"Stemmed text:\", stemmed_tokens)\n",
    "print(\"Lemmatized text:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopword removal is a common technique used in NLP to preprocess and clean text data by removing words that are considered to be of little or no value in terms of conveying meaning or information. These words are called \"stopwords\" and they include common words such as \"the\", \"a\", \"an\", \"and\", \"or\", \"but\", and so on.\n",
    "\n",
    "The purpose of stopword removal in NLP is to improve the accuracy and efficiency of text analysis and processing by reducing the noise and complexity of the data. Stopwords are often used to form grammatical structures in a sentence, but they do not carry much meaning or relevance to the main topic or theme of the text. So by removing these words, we can reduce the dimensionality of the text data, improve the performance of machine learning models, and speed up the processing of text data. NLTK has a predefined list of stopwords for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English stopwords in NLTK.\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment for Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task (which is your assignment) is to write functions to do the following:**\n",
    "- Since we want to use our text to make a model later on, we need to preprocess it. Add a `tokens` column to the `train_df` dataframe with the text tokenized, then lemmatize those tokens.\n",
    "    - Hint: Use the `pandas.Series.apply` function with the imported `nltk.tokenize.word_tokenize` function. This might take a moment. Recall that you can use the `pd.Series.apply?` syntax in a code cell for more information.\n",
    "    - Hint: use the `nltk.stem.WordNetLemmatizer.lemmatize` function to lemmatize a token.\n",
    "- To see what the most used words per class are, create a new, seperate dataframe with the 5 most used words per class. Sort the resulting dataframe ascending on the `class` and descending on the `count`.\n",
    "    - Hint: use the `pandas.Series.apply` and `str.isalpha()` functions to filter out non-alphabetical tokens.\n",
    "    - Hint: use the `pandas.DataFrame.explode` to create one row per class and token.\n",
    "    - Hint: use `pandas.DataFrame.groupby` with `.size()` afterwards or `pandas.DataFrame.pivot_table` with `size` as the `aggfunc` to obtain the occurences per class.\n",
    "    - Hint: use the `pandas.Series.reset_index` function to obtain a dataframe with `[class, tokens, count]` as the columns.\n",
    "    - Hint: use the `pandas.DataFrame.sort_values` function for sorting a dataframe.\n",
    "    - Hint: use the `pandas.DataFrame.groupby` and `pandas.DataFrame.head` functions to get the first 5 rows per class.\n",
    "- Remove the stopwords from the `tokens` column in the `train_df` dataframe. Do the most used tokens say something about the class now?\n",
    "    - Hint: once again, you can use the `pandas.Series.apply` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(df):\n",
    "    \"\"\"\n",
    "    Tokenize and lemmatize the text in the dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing at least the text column.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with the added tokens column.\n",
    "    \"\"\"\n",
    "    # Copy the dataframe to avoid editing the original one.\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    # Apply the tokenizer to create the tokens column.\n",
    "    df['tokens'] = df['text'].apply(word_tokenize)\n",
    "    \n",
    "    # Apply the lemmatizer on every word in the tokens list.\n",
    "    df['tokens'] = df['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "    return df\n",
    "\n",
    "\n",
    "def most_used_words(df, token_col='tokens'):\n",
    "    \"\"\"\n",
    "    Generate a dataframe with the 5 most used words per class, and their count.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing at least the class and tokens columns.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with 5 rows per class, and an added 'count' column.\n",
    "        The dataframe is sorted in ascending order on the class and in descending order on the count.\n",
    "    \"\"\"\n",
    "    # Copy the dataframe to avoid editing the original one.\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    # Filter out non-words\n",
    "    df[token_col] = df[token_col].apply(lambda tokens: [token for token in tokens if token.isalpha()])\n",
    "    \n",
    "    # Explode the tokens so that every token gets its own row.\n",
    "    df = df.explode(token_col)\n",
    "    \n",
    "    # Option 1: groupby on class and token, get the size of how many rows per item, \n",
    "    # add that as a column.\n",
    "    counts = df.groupby(['class', token_col]).size().reset_index(name='count')\n",
    "    \n",
    "    # Option 2: make a pivot table based on the class and token based on how many\n",
    "    # rows per combination there are , add counts as a column.\n",
    "    # counts = counts.pivot_table(index=['class', 'tokens'], aggfunc='size').reset_index(name='count')\n",
    "    \n",
    "    # Sort the values on the class and count, get only the first 5 rows per class.\n",
    "    counts = counts.sort_values(['class', 'count'], ascending=[True, False]).groupby('class').head()\n",
    "\n",
    "    return counts\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    \"\"\"\n",
    "    Remove stopwords from the tokens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing at least the tokens column.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with stopwords removed from the tokens column.\n",
    "    \"\"\"\n",
    "    # Copy the dataframe to avoid editing the original one.\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    # Using a set for quicker lookups.\n",
    "    stopwords_set = set(stopwords_list)\n",
    "    \n",
    "    # Filter stopwords from tokens.\n",
    "    df['tokens'] = df['tokens'].apply(lambda tokens: [token for token in tokens if token.lower() not in stopwords_set])\n",
    "    \n",
    "    return df\n",
    "\n",
    "tok = tokenize_and_lemmatize(train_df)\n",
    "train_df = remove_stopwords(tok)\n",
    "display(most_used_words(tok), most_used_words(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Another option: spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is another library used to perform various NLP tasks like tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and much more. It provides pre-trained models for different languages and domains, which can be used as-is but also can be fine-tuned on a specific task or domain.\n",
    "\n",
    "In an object-oriented way, spaCy can be thought of as a collection of classes and objects that work together to perform NLP tasks. Some of the important functions and classes in spaCy include:\n",
    "\n",
    "- `nlp`: The core function that provides the main functionality of spaCy. It is used to process text and create a `Doc` object.\n",
    "- [`Doc`](https://spacy.io/api/doc): A container for accessing linguistic annotations like tokens, part-of-speech tags, named entities, and dependency parse information. It is created by the `nlp` function and represents a processed document.\n",
    "- [`Token`](https://spacy.io/api/token): An object representing a single token in a `Doc` object. It contains information like the token text, part-of-speech tag, lemma, embedding, and much more.\n",
    "\n",
    "When a text is processed by spaCy, it is first passed to the nlp function, which uses the loaded model to tokenize the text and applies various linguistic annotations like part-of-speech tagging, named entity recognition, and dependency parsing in the background. The resulting annotations are stored in a Doc object, which can be accessed and manipulated using various methods and attributes. For example, the Doc object can be iterated over to access each Token object in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small English model in spaCy.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text using spaCy.\n",
    "doc = nlp(text)\n",
    "\n",
    "# This becomes a spaCy Doc object, which prints nicely as the original string.\n",
    "print(type(doc) , doc)\n",
    "\n",
    "# We can iterate over the tokens in the Doc, since it has already been tokenized underneath.\n",
    "print(type(doc[0]))\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a lot of processing has already been done, we can also directly access multiple attributes of the `Token` objects. For example, we can directly access the lemma of the token with `Token.lemma_` and check if a token is a stop word with `Token.is_stop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[0].lemma_, type(doc[0].lemma_), doc[0].is_stop, type(doc[0].is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task (which is your assignment) is to write functions to do the following:**\n",
    "- Add a `doc` column to the `train_df` dataframe containing the `Doc` representation of that row's `text`.\n",
    "- Add a `spacy_tokens` column containing the to the `train_df` dataframe containing a list of lemmatized tokens (strings). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Unsupervised Learning - Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use LDA to transform preprocessed text into features\n",
    "- Run simple Kmeans to make clusters\n",
    "- Let student pick amount of clusters (elbow method)\n",
    "- Evaluate using adjusted_mutual_info_score and adjusted_rand_score\n",
    "- Similar to an assignment from the Applied ML course these students had last year, but LDA was cut for their year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed text data\n",
    "data = pd.read_csv('preprocessed_text.csv')\n",
    "\n",
    "# Define the number of topics to extract with LDA\n",
    "num_topics = 10\n",
    "\n",
    "# Convert preprocessed text to features using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data['preprocessed_text'])\n",
    "\n",
    "# Fit LDA to the feature matrix\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, max_iter=10, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Extract the topic proportions for each document\n",
    "doc_topic_proportions = lda.transform(X)\n",
    "\n",
    "# Determine the optimal number of clusters with KMeans using the elbow method\n",
    "k_range = range(2, 11)\n",
    "sse = []\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=100)\n",
    "    kmeans.fit(doc_topic_proportions)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot the elbow curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(k_range, sse)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()\n",
    "\n",
    "# Let the user select the number of clusters\n",
    "num_clusters = int(input(\"Enter the number of clusters: \"))\n",
    "\n",
    "# Cluster the documents using KMeans\n",
    "kmeans = KMeans(n_clusters=num_clusters, max_iter=100)\n",
    "kmeans.fit(doc_topic_proportions)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Evaluate the clustering using adjusted mutual information score and adjusted rand score\n",
    "ami_score = adjusted_mutual_info_score(data['category'], cluster_labels)\n",
    "ari_score = adjusted_rand_score(data['category'], cluster_labels)\n",
    "\n",
    "print(f\"Adjusted mutual information score: {ami_score:.2f}\")\n",
    "print(f\"Adjusted rand score: {ari_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show code to make embeddings based on pre-processed text using both NLTK and spaCy\n",
    "- Assignment: Let student apply it to dataframe\n",
    "\n",
    "Sources:\n",
    "- https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/\n",
    "- https://www.kaggle.com/code/vukglisovic/classification-combining-lda-and-word2vec/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load preprocessed text data\n",
    "data = pd.read_csv('preprocessed_text.csv')\n",
    "\n",
    "# Define the preprocessing functions\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "data['tokens'] = data['preprocessed_text'].apply(preprocess_text)\n",
    "\n",
    "# Train a Word2Vec model on the preprocessed text data\n",
    "model = Word2Vec(data['tokens'], size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the word embedding for a specific word\n",
    "embedding = model.wv['word']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load preprocessed text data\n",
    "# data = pd.read_csv('preprocessed_text.csv')\n",
    "\n",
    "# # Define the preprocessing function\n",
    "# def preprocess_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "#     return tokens\n",
    "\n",
    "# # Apply the preprocessing function to the text data\n",
    "# data['tokens'] = data['preprocessed_text'].apply(preprocess_text)\n",
    "\n",
    "# Get the word embedding for a specific word\n",
    "embedding = nlp('.').vector\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Supervised Learning - Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the word embeddings features, train a small neural net \n",
    "- Don't give the full torch code, only one layer to let them do something with torch\n",
    "- Hyperparameter tuning (either some hinted ones or see if Ray Tune is worth it for this task)\n",
    "- Evaluate using confusion matrix against true features\n",
    "\n",
    "Sources:\n",
    "- https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load preprocessed text data with word embeddings as features\n",
    "df = pd.read_csv('preprocessed_data.csv')\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['topic'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a simple neural network with one hidden layer\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = len(X_train[0])\n",
    "hidden_dim = 100\n",
    "output_dim = len(y_train.unique())\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Initialize model, optimizer and loss function\n",
    "model = Net(input_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(len(X_train)):\n",
    "        # Convert input and target to tensors\n",
    "        input_tensor = torch.tensor(X_train.iloc[i])\n",
    "        target_tensor = torch.tensor(y_train.iloc[i])\n",
    "        \n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target_tensor)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Print every 100 batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(X_test)):\n",
    "        # Convert input to tensor\n",
    "        input_tensor = torch.tensor(X_test.iloc[i])\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        # Get predicted class\n",
    "        _, predicted = torch.max(output.data, 0)\n",
    "        y_pred.append(predicted.item())\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(conf_mat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
